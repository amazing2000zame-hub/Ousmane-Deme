---
phase: 04-autonomous-monitoring-remediation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/monitor/types.ts
  - src/monitor/state-tracker.ts
  - src/monitor/thresholds.ts
  - src/monitor/poller.ts
  - src/monitor/index.ts
  - src/db/schema.ts
  - src/db/migrate.ts
  - src/db/memory.ts
autonomous: true

must_haves:
  truths:
    - "Monitor service starts alongside emitter and polls cluster state on tiered intervals"
    - "State changes (VM running->stopped, node online->offline) are detected and emitted as events"
    - "Threshold violations (disk >90%, RAM >95%) are detected and emitted as alerts"
    - "Autonomy actions audit table exists in SQLite and accepts inserts"
    - "Monitor polling is offset from emitter polling to avoid API thundering herd"
  artifacts:
    - path: "src/monitor/types.ts"
      provides: "AutonomyLevel enum, Incident, StateChange, ThresholdViolation, MonitorEvent types"
      exports: ["AutonomyLevel", "Incident", "StateChange", "ThresholdViolation"]
    - path: "src/monitor/state-tracker.ts"
      provides: "Previous state storage for change detection (nodes + VMs)"
      exports: ["StateTracker"]
    - path: "src/monitor/thresholds.ts"
      provides: "Threshold definitions and evaluation logic"
      exports: ["evaluateThresholds", "THRESHOLDS"]
    - path: "src/monitor/poller.ts"
      provides: "Tiered polling functions that poll Proxmox API and run detection"
      exports: ["pollCritical", "pollImportant", "pollRoutine", "pollBackground"]
    - path: "src/monitor/index.ts"
      provides: "Monitor lifecycle management"
      exports: ["startMonitor", "stopMonitor"]
    - path: "src/db/schema.ts"
      provides: "autonomyActions table schema added to existing schema"
      contains: "autonomyActions"
    - path: "src/db/memory.ts"
      provides: "CRUD operations for autonomy actions"
      contains: "saveAutonomyAction"
  key_links:
    - from: "src/monitor/poller.ts"
      to: "src/clients/proxmox.ts"
      via: "getAnyClient() for Proxmox API polling"
      pattern: "getAnyClient"
    - from: "src/monitor/poller.ts"
      to: "src/monitor/state-tracker.ts"
      via: "compare current poll with tracked state"
      pattern: "stateTracker\\.update"
    - from: "src/monitor/poller.ts"
      to: "src/monitor/thresholds.ts"
      via: "evaluateThresholds() on polled node data"
      pattern: "evaluateThresholds"
    - from: "src/monitor/index.ts"
      to: "src/monitor/poller.ts"
      via: "setInterval calling poll functions"
      pattern: "setInterval.*poll"
    - from: "src/monitor/poller.ts"
      to: "src/db/memory.ts"
      via: "saveEvent() for detected conditions"
      pattern: "memoryStore\\.saveEvent"
---

<objective>
Build the autonomous monitoring detection backbone -- the monitor service that polls cluster state on tiered intervals, detects state changes (VM crashes, node offline) and threshold violations (disk/RAM/CPU), and emits events to the /events Socket.IO namespace. Also add the autonomy_actions audit log table to SQLite.

Purpose: This is the sensing layer of the autonomous monitoring system. Without detection, there is nothing to remediate. This plan builds the infrastructure that Plan 02 (runbooks + guardrails) will act upon.

Output: A running monitor service that detects cluster problems and emits events. The audit log table ready for Plan 02 to write remediation records.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-autonomous-monitoring-remediation/04-RESEARCH.md

Key existing files to reference:
@jarvis-backend/src/realtime/emitter.ts (polling pattern to follow, types to reuse)
@jarvis-backend/src/db/schema.ts (add autonomyActions table here)
@jarvis-backend/src/db/migrate.ts (add CREATE TABLE for autonomyActions)
@jarvis-backend/src/db/memory.ts (add CRUD operations for autonomy actions)
@jarvis-backend/src/clients/proxmox.ts (getAnyClient for polling)
@jarvis-backend/src/index.ts (wire startMonitor/stopMonitor into lifecycle)
@jarvis-backend/src/config.ts (cluster node definitions)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Monitor types, audit log schema, and memory store extensions</name>
  <files>
    src/monitor/types.ts
    src/db/schema.ts
    src/db/migrate.ts
    src/db/memory.ts
  </files>
  <action>
    **1. Create `src/monitor/types.ts`** with all shared types for the monitor domain:

    - `AutonomyLevel` enum: L0_OBSERVE=0, L1_ALERT=1, L2_RECOMMEND=2, L3_ACT_REPORT=3, L4_ACT_SILENT=4
    - `ConditionType` string union: 'NODE_UNREACHABLE' | 'VM_CRASHED' | 'CT_CRASHED' | 'DISK_HIGH' | 'DISK_CRITICAL' | 'RAM_CRITICAL' | 'RAM_HIGH' | 'CPU_HIGH' | 'SERVICE_DOWN' | 'TEMP_HIGH'
    - `StateChange` interface: { type: ConditionType; target: string; node: string; previousState: string; currentState: string; timestamp: string; details?: Record<string, unknown> }
    - `ThresholdViolation` interface: { type: ConditionType; node: string; metric: string; value: number; threshold: number; severity: 'warning' | 'error' | 'critical'; timestamp: string }
    - `Incident` interface: { id: string; key: string; type: ConditionType; node: string; target: string; detectedAt: string; details: Record<string, unknown> }
    - `MonitorEvent` interface: { type: 'state_change' | 'threshold' | 'remediation'; incident: Incident; change?: StateChange; violation?: ThresholdViolation }

    **2. Add `autonomyActions` table to `src/db/schema.ts`** (append after existing tables):

    ```typescript
    export const autonomyActions = sqliteTable('autonomy_actions', {
      id: integer('id').primaryKey({ autoIncrement: true }),
      timestamp: text('timestamp').notNull().default(sql`(datetime('now'))`),
      incidentKey: text('incident_key').notNull(),
      incidentId: text('incident_id').notNull(),
      runbookId: text('runbook_id').notNull(),
      condition: text('condition').notNull(),
      action: text('action').notNull(),
      actionArgs: text('action_args'),
      result: text('result', { enum: ['success', 'failure', 'blocked', 'escalated'] }).notNull(),
      resultDetails: text('result_details'),
      verificationResult: text('verification_result'),
      autonomyLevel: integer('autonomy_level').notNull(),
      node: text('node'),
      attemptNumber: integer('attempt_number').notNull().default(1),
      escalated: integer('escalated', { mode: 'boolean' }).notNull().default(false),
      emailSent: integer('email_sent', { mode: 'boolean' }).notNull().default(false),
    });
    ```

    **3. Add CREATE TABLE to `src/db/migrate.ts`** fallback SQL block -- append after the preferences table creation:

    ```sql
    CREATE TABLE IF NOT EXISTS autonomy_actions (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      timestamp TEXT NOT NULL DEFAULT (datetime('now')),
      incident_key TEXT NOT NULL,
      incident_id TEXT NOT NULL,
      runbook_id TEXT NOT NULL,
      condition TEXT NOT NULL,
      action TEXT NOT NULL,
      action_args TEXT,
      result TEXT NOT NULL,
      result_details TEXT,
      verification_result TEXT,
      autonomy_level INTEGER NOT NULL,
      node TEXT,
      attempt_number INTEGER NOT NULL DEFAULT 1,
      escalated INTEGER NOT NULL DEFAULT 0,
      email_sent INTEGER NOT NULL DEFAULT 0
    );

    CREATE INDEX IF NOT EXISTS idx_autonomy_actions_timestamp ON autonomy_actions(timestamp);
    CREATE INDEX IF NOT EXISTS idx_autonomy_actions_incident_key ON autonomy_actions(incident_key);
    CREATE INDEX IF NOT EXISTS idx_autonomy_actions_result ON autonomy_actions(result);
    ```

    **4. Add autonomy action CRUD operations to `src/db/memory.ts`**:

    Import `autonomyActions` from schema. Add these functions and export them on the `memoryStore` object:

    - `saveAutonomyAction(input)` -- insert into autonomy_actions, return the row
    - `getAutonomyActions(limit = 50)` -- get recent actions ordered by timestamp desc
    - `getActionsByIncidentKey(key, limit = 20)` -- filter by incident_key
    - `getAttemptCountSince(incidentKey, sinceTimestamp)` -- count actions matching key after timestamp (for rate limiting in Plan 02)
    - `cleanupOldActions(olderThanDays = 30)` -- delete actions older than N days (for background cleanup)

    Input type for saveAutonomyAction should match the table columns minus id and timestamp (auto-generated).
  </action>
  <verify>
    - `npx tsc --noEmit` passes with no type errors
    - The autonomy_actions table is created when the server starts (check via sqlite3 data/jarvis.db ".tables")
    - All new memoryStore functions are exported and callable
  </verify>
  <done>
    - autonomyActions schema defined in schema.ts
    - CREATE TABLE IF NOT EXISTS added to migrate.ts
    - 5 CRUD functions added to memoryStore
    - AutonomyLevel, Incident, StateChange, ThresholdViolation types exported from types.ts
    - TypeScript compilation succeeds
  </done>
</task>

<task type="auto">
  <name>Task 2: State tracker, threshold evaluator, pollers, and monitor lifecycle</name>
  <files>
    src/monitor/state-tracker.ts
    src/monitor/thresholds.ts
    src/monitor/poller.ts
    src/monitor/index.ts
    src/index.ts
  </files>
  <action>
    **1. Create `src/monitor/state-tracker.ts`**:

    A class `StateTracker` that maintains the last-known state of nodes and VMs in memory (not SQLite -- this is hot-path data).

    ```typescript
    interface TrackedNode { status: string; lastSeen: number; }
    interface TrackedVM { status: string; node: string; lastSeen: number; }

    class StateTracker {
      private nodes = new Map<string, TrackedNode>();
      private vms = new Map<number, TrackedVM>();

      updateNodes(current: NodeData[]): StateChange[]
      // For each node in current:
      //   - If not tracked yet, add to map (no change emitted)
      //   - If tracked and status changed (e.g. 'online' -> 'offline'), emit StateChange
      //   - Update lastSeen timestamp
      // Return list of StateChange objects

      updateVMs(current: VMData[]): StateChange[]
      // For each VM in current:
      //   - If not tracked yet, add to map (no change emitted)
      //   - If tracked and status changed (e.g. 'running' -> 'stopped'), emit StateChange with type VM_CRASHED (or CT_CRASHED based on VM type)
      //   - Update lastSeen timestamp
      // Return list of StateChange objects

      getTrackedNodeStatus(name: string): string | undefined
      getTrackedVMStatus(vmid: number): string | undefined
    }
    ```

    Import NodeData and VMData types from `../realtime/emitter.js` (they are already exported from there).

    **IMPORTANT:** Only emit VM_CRASHED/CT_CRASHED when a VM transitions from 'running' to 'stopped'. Do NOT emit for VMs that were already stopped when monitoring began. The initial pass populates state without emitting changes.

    **2. Create `src/monitor/thresholds.ts`**:

    Define threshold constants and an evaluateThresholds function:

    ```typescript
    interface Threshold {
      metric: string;
      operator: '>';
      value: number;
      severity: 'warning' | 'error' | 'critical';
      condition: ConditionType;
    }

    const THRESHOLDS: Threshold[] = [
      { metric: 'disk_percent', operator: '>', value: 90, severity: 'error', condition: 'DISK_HIGH' },
      { metric: 'disk_percent', operator: '>', value: 95, severity: 'critical', condition: 'DISK_CRITICAL' },
      { metric: 'mem_percent', operator: '>', value: 95, severity: 'critical', condition: 'RAM_CRITICAL' },
      { metric: 'mem_percent', operator: '>', value: 85, severity: 'warning', condition: 'RAM_HIGH' },
      { metric: 'cpu_percent', operator: '>', value: 95, severity: 'warning', condition: 'CPU_HIGH' },
    ];

    function evaluateThresholds(nodes: NodeData[]): ThresholdViolation[]
    // For each node, calculate disk_percent (disk/maxdisk*100), mem_percent (mem/maxmem*100), cpu_percent (cpu*100)
    // Check each threshold; return list of violations
    ```

    Use a `Set<string>` to track active violations (key = `${condition}:${node}`) and only emit NEW violations (not re-emit every poll cycle). Clear violations when the metric drops below threshold.

    **3. Create `src/monitor/poller.ts`**:

    Import getAnyClient from proxmox client. Import StateTracker and evaluateThresholds.

    Four async poll functions, each wrapped in try/catch (never throw):

    - `pollCritical(eventsNs, stateTracker)`: Poll nodes (getClusterResources('node')) and VMs (getClusterResources('vm')). Run stateTracker.updateNodes() and stateTracker.updateVMs(). For each StateChange detected:
      - Call memoryStore.saveEvent() with type='alert', severity based on condition, source='monitor', node, summary describing the change
      - Emit to eventsNs as `'event'` with JarvisEvent format: { id: crypto.randomUUID(), type: 'alert', severity, title, message, node, timestamp }
      - Return the changes for the runbook engine (Plan 02 will use these)

    - `pollImportant(eventsNs, thresholdEvaluator)`: Poll nodes. Run evaluateThresholds(). For new threshold violations:
      - Call memoryStore.saveEvent() with type='alert', severity from violation
      - Emit to eventsNs as `'event'`

    - `pollRoutine(eventsNs)`: Placeholder for service health checks and temperature monitoring. For now, just log that routine poll ran. This will be extended later.

    - `pollBackground(eventsNs)`: Placeholder for storage capacity planning, backup freshness, and audit log cleanup. Call memoryStore.cleanupOldActions(30) to prune old audit records.

    Each function returns its detected incidents (StateChange[] or ThresholdViolation[]) so the runbook engine in Plan 02 can act on them.

    **IMPORTANT:** All polling functions must use `Promise.allSettled()` for parallel operations. Individual node failures must not prevent other nodes from being monitored.

    **4. Create `src/monitor/index.ts`**:

    Module lifecycle management:

    ```typescript
    import type { Namespace } from 'socket.io';

    const intervals: ReturnType<typeof setInterval>[] = [];
    let running = false;

    const POLL_INTERVALS = {
      critical: 12_000,     // 12s (offset from emitter's 10s)
      important: 32_000,    // 32s (offset from emitter's 30s)
      routine: 300_000,     // 5 min
      background: 1_800_000, // 30 min
    };

    function startMonitor(eventsNs: Namespace): void
    // Create StateTracker and ThresholdEvaluator instances
    // Start 4 setInterval loops, each calling the appropriate poll function
    // Initial delay of 5 seconds before first poll (let emitter populate first)
    // Set running = true
    // Log: '[Monitor] Autonomous monitoring service started'

    function stopMonitor(): void
    // Clear all intervals
    // Set running = false
    // Log: '[Monitor] Autonomous monitoring service stopped'

    function isMonitorRunning(): boolean
    // Return running state
    ```

    Note the offset intervals: 12s instead of 10s, 32s instead of 30s -- this avoids the emitter and monitor hitting the Proxmox API at the same instant.

    **5. Wire into `src/index.ts`**:

    - Import `startMonitor` and `stopMonitor` from `./monitor/index.js`
    - After `startEmitter(clusterNs)`, add: `startMonitor(eventsNs);`
    - In the `shutdown()` function, before `stopEmitter()`, add: `stopMonitor();`
    - Add console.log: `'[Monitor] Autonomous monitoring service started'`

    **CRITICAL SAFETY:** Every setInterval callback MUST be wrapped in try/catch. An error in the monitor must NEVER crash the backend process. Use this pattern:
    ```typescript
    setInterval(() => { pollCritical(eventsNs, stateTracker).catch(err => console.error('[Monitor] Critical poll error:', err)); }, POLL_INTERVALS.critical);
    ```
  </action>
  <verify>
    - `npx tsc --noEmit` passes with no type errors
    - Server starts without errors: `npx tsx src/index.ts` (check console output shows "[Monitor] Autonomous monitoring service started")
    - After 12-15 seconds, the monitor emits state data (check logs for poll activity)
    - If a VM is running, the state tracker records it. If the same VM is still running next poll, no StateChange is emitted (only transitions trigger changes)
  </verify>
  <done>
    - StateTracker correctly detects node and VM state transitions
    - Threshold evaluator detects disk/RAM/CPU violations
    - 4 tiered pollers run at offset intervals (12s/32s/5min/30min)
    - Monitor starts and stops cleanly with server lifecycle
    - Detected conditions are saved as events and emitted to /events namespace
    - Monitor errors are caught and logged without crashing the backend
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` -- zero type errors
2. Server starts cleanly and logs both emitter and monitor initialization
3. The /events namespace receives monitor-sourced events (visible in frontend ActivityFeed)
4. SQLite has autonomy_actions table: `sqlite3 data/jarvis.db ".tables"` includes autonomy_actions
5. Monitor polling logs appear at the expected intervals (12s critical, 32s important)
6. No duplicate state change events on consecutive polls when nothing changed
</verification>

<success_criteria>
- Monitor service runs alongside the existing emitter without conflicts
- State changes (node offline, VM stopped unexpectedly) generate events within 12 seconds
- Threshold violations generate alerts with correct severity levels
- Audit log table is created and CRUD operations work
- Monitor errors do not crash the backend process
</success_criteria>

<output>
After completion, create `.planning/phases/04-autonomous-monitoring-remediation/04-01-SUMMARY.md`
</output>
