---
phase: 04-autonomous-monitoring-remediation
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/monitor/guardrails.ts
  - src/monitor/runbooks.ts
  - src/monitor/reporter.ts
  - src/monitor/poller.ts
  - src/monitor/index.ts
  - src/api/routes.ts
autonomous: true

must_haves:
  truths:
    - "When a VM crashes, Jarvis automatically restarts it via executeTool and verifies recovery"
    - "When a node goes offline, Jarvis sends Wake-on-LAN and waits for recovery"
    - "After 3 failed remediation attempts for the same issue in 1 hour, Jarvis stops and sends a diagnostic email"
    - "A kill switch toggle disables all autonomous actions -- Jarvis still monitors but does not act"
    - "Jarvis never remediates more than 1 node simultaneously (blast radius control)"
    - "Every autonomous action is recorded in the autonomy_actions audit log"
    - "Remediation emails are sent via SSH to agent1 email service"
  artifacts:
    - path: "src/monitor/guardrails.ts"
      provides: "Kill switch check, rate limiter, blast radius control, escalation logic"
      exports: ["checkGuardrails", "isKillSwitchActive", "recordAttempt", "getActiveRemediationCount"]
    - path: "src/monitor/runbooks.ts"
      provides: "Runbook definitions and execution engine"
      exports: ["RUNBOOKS", "executeRunbook", "findRunbook"]
    - path: "src/monitor/reporter.ts"
      provides: "Email report generation and sending via agent1"
      exports: ["sendRemediationEmail", "sendEscalationEmail"]
    - path: "src/api/routes.ts"
      provides: "GET /api/monitor/status and PUT /api/monitor/killswitch endpoints"
      contains: "/api/monitor"
  key_links:
    - from: "src/monitor/runbooks.ts"
      to: "src/mcp/server.ts"
      via: "executeTool() for all remediation actions"
      pattern: "executeTool\\("
    - from: "src/monitor/guardrails.ts"
      to: "src/db/memory.ts"
      via: "getPreference('autonomy.killSwitch') for kill switch state"
      pattern: "memoryStore\\.getPreference"
    - from: "src/monitor/poller.ts"
      to: "src/monitor/runbooks.ts"
      via: "executeRunbook() called when state changes are detected"
      pattern: "executeRunbook"
    - from: "src/monitor/reporter.ts"
      to: "src/clients/ssh.ts"
      via: "execOnNode(agent1) to send email via emailService"
      pattern: "execOnNode\\("
    - from: "src/api/routes.ts"
      to: "src/db/memory.ts"
      via: "getPreference/setPreference for kill switch state"
      pattern: "autonomy\\.killSwitch"
---

<objective>
Build the remediation engine -- runbooks that map detected conditions to MCP tool actions, guardrails that enforce safety (kill switch, rate limits, blast radius, escalation), and email reporting. Wire the detection layer from Plan 01 to the action layer so Jarvis can autonomously fix problems.

Purpose: This transforms the monitor from a passive observer into an active remediation system. Detected problems are matched to runbooks, checked against guardrails, executed via the existing executeTool pipeline, verified, and reported.

Output: A working autonomous remediation pipeline that can restart crashed VMs, wake offline nodes, and restart failed services -- with full safety controls.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-autonomous-monitoring-remediation/04-RESEARCH.md
@.planning/phases/04-autonomous-monitoring-remediation/04-01-SUMMARY.md

Key existing files to reference:
@jarvis-backend/src/mcp/server.ts (executeTool -- the remediation execution path)
@jarvis-backend/src/safety/tiers.ts (TOOL_TIERS -- start_vm is YELLOW, stop_vm is RED, wake_node is YELLOW)
@jarvis-backend/src/clients/ssh.ts (execOnNode for email sending to agent1)
@jarvis-backend/src/db/memory.ts (saveEvent, saveAutonomyAction, getPreference, setPreference)
@jarvis-backend/src/api/routes.ts (add monitor status and kill switch endpoints)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Guardrails, runbook engine, and email reporter</name>
  <files>
    src/monitor/guardrails.ts
    src/monitor/runbooks.ts
    src/monitor/reporter.ts
  </files>
  <action>
    **1. Create `src/monitor/guardrails.ts`**:

    Implements all pre-execution safety checks for autonomous actions.

    ```typescript
    import { memoryStore } from '../db/memory.js';

    interface GuardrailResult {
      allowed: boolean;
      reason?: string;
    }

    // In-memory sliding window rate limiter
    const attemptLog = new Map<string, number[]>();

    // Track active remediations (node -> start timestamp)
    const activeRemediations = new Map<string, number>();
    ```

    Functions to implement:

    - `isKillSwitchActive(): boolean` -- reads `memoryStore.getPreference('autonomy.killSwitch')`, returns true if value === 'true'

    - `getCurrentAutonomyLevel(): number` -- reads `memoryStore.getPreference('autonomy.level')`, returns parsed int, defaults to 3 (L3_ACT_REPORT)

    - `getAttemptCount(key: string, windowMs: number): number` -- sliding window count from attemptLog Map. Filter timestamps within window, clean up old entries.

    - `recordAttempt(key: string): void` -- push Date.now() to attemptLog for key

    - `markRemediationActive(node: string): void` -- add node to activeRemediations with current timestamp

    - `markRemediationComplete(node: string): void` -- remove node from activeRemediations

    - `getActiveRemediationCount(): number` -- return activeRemediations.size. Also clean up stale entries older than 10 minutes (safety net for stuck remediations).

    - `checkGuardrails(incidentKey: string, node: string, requiredLevel: number): GuardrailResult` -- checks in order:
      1. Kill switch active? -> { allowed: false, reason: 'Kill switch is active' }
      2. Rate limit: getAttemptCount(incidentKey, 3_600_000) >= 3? -> { allowed: false, reason: 'Rate limit exceeded (3/hour) -- escalating' }
      3. Blast radius: getActiveRemediationCount() > 0? -> { allowed: false, reason: 'Another remediation is in progress (blast radius control)' }
      4. Autonomy level: getCurrentAutonomyLevel() < requiredLevel? -> { allowed: false, reason: 'Current autonomy level insufficient' }
      5. All pass -> { allowed: true }

    **2. Create `src/monitor/runbooks.ts`**:

    Defines runbook mappings and the execution engine.

    ```typescript
    import { executeTool } from '../mcp/server.js';
    import { memoryStore } from '../db/memory.js';
    import { getAnyClient } from '../clients/proxmox.js';
    import { AutonomyLevel, type Incident, type StateChange, type ConditionType } from './types.js';
    import { checkGuardrails, recordAttempt, markRemediationActive, markRemediationComplete } from './guardrails.js';
    import { sendRemediationEmail, sendEscalationEmail } from './reporter.js';
    ```

    **Runbook definitions** (array of Runbook objects):

    ```typescript
    interface RunbookAction {
      tool: string;
      argsBuilder: (incident: Incident) => Record<string, unknown>;
    }

    interface Runbook {
      id: string;
      name: string;
      trigger: ConditionType;
      autonomyLevel: AutonomyLevel;
      action: RunbookAction;
      verifyDelayMs: number;  // Wait before verifying
      cooldownMs: number;
    }
    ```

    Runbooks to define:

    1. `vm-crashed-restart`: trigger=VM_CRASHED, level=L3_ACT_REPORT, tool=start_vm, args: { node: incident.node, vmid: parseInt(incident.target), confirmed: true }. Verify delay: 15s. Cooldown: 60s.
       NOTE: start_vm is YELLOW tier (no confirmation needed), but pass confirmed:true anyway for consistency.

    2. `ct-crashed-restart`: trigger=CT_CRASHED, level=L3_ACT_REPORT, tool=start_container, args: { node: incident.node, vmid: parseInt(incident.target), confirmed: true }. Verify delay: 10s. Cooldown: 60s.

    3. `node-unreachable-wol`: trigger=NODE_UNREACHABLE, level=L3_ACT_REPORT, tool=wake_node, args: { node: incident.node }. Verify delay: 60s (WOL takes time). Cooldown: 120s.

    - `findRunbook(conditionType: ConditionType): Runbook | undefined` -- find the first matching runbook for a condition

    - `executeRunbook(incident: Incident, eventsNs: Namespace): Promise<void>` -- the core remediation pipeline:

      1. Find matching runbook via findRunbook(incident.type)
      2. If no runbook, return (condition has no automated fix)
      3. Check guardrails: checkGuardrails(incident.key, incident.node, runbook.autonomyLevel)
      4. If guardrails block:
         - If reason includes 'Rate limit' -> call escalation: send escalation email, save autonomy action with result='escalated'
         - Save event describing the block. Emit to eventsNs
         - Return
      5. Record attempt: recordAttempt(incident.key)
      6. Mark remediation active: markRemediationActive(incident.node)
      7. Emit "remediation starting" event to eventsNs
      8. Execute tool: `const result = await executeTool(runbook.action.tool, runbook.action.argsBuilder(incident), 'monitor')`
      9. Wait for verify delay: `await new Promise(r => setTimeout(r, runbook.verifyDelayMs))`
      10. Verify: Re-poll the specific resource (VM status or node status) to check recovery
      11. Save autonomy action record: memoryStore.saveAutonomyAction({ incidentKey, incidentId, runbookId, condition, action, result: success ? 'success' : 'failure', ... })
      12. Emit result event to eventsNs (resolved or failed)
      13. If success: send remediation email (success report)
      14. If failure: save event, check if attempt count now >= 3, if so escalate
      15. Mark remediation complete: markRemediationComplete(incident.node)

      **CRITICAL:** Wrap the ENTIRE pipeline in try/catch/finally. The finally block MUST call markRemediationComplete() to prevent blast radius deadlocks.

      **IMPORTANT:** Kill switch is double-checked -- once in checkGuardrails (step 3) and once immediately before executeTool (step 8). If kill switch was toggled between detection and execution, the action is aborted.

    **3. Create `src/monitor/reporter.ts`**:

    Email sending via SSH to agent1 (192.168.1.61) using the existing /opt/agent email service.

    ```typescript
    import { execOnNode } from '../clients/ssh.js';

    const AGENT1_HOST = '192.168.1.61';
    ```

    Email rate limiting: Track last email sent timestamp. Do not send more than 1 email per 5 minutes (300_000 ms). Skip silently if rate limited.

    Functions:

    - `sendRemediationEmail(incident, result, success)` -- sends HTML email with:
      - Subject: `[Jarvis] ${success ? 'Resolved' : 'Failed'}: ${incident.type} on ${incident.node}`
      - Body: HTML table with incident details, action taken, result, timestamp
      - Uses SSH to agent1: `cd /opt/agent && node -e "require('dotenv').config(); const es = require('./src/services/emailService'); es.init(); es.sendNotification('amazing2000zame@gmail.com', SUBJECT, HTML).then(() => process.exit(0)).catch(e => { console.error(e); process.exit(1); });"`
      - Escape single quotes in HTML/subject for shell safety

    - `sendEscalationEmail(incident, attemptCount)` -- sends urgent email:
      - Subject: `[Jarvis] ESCALATION: ${incident.type} on ${incident.node} -- ${attemptCount} failed attempts`
      - Body: HTML with red header, incident history, recommendation to investigate manually
      - This email bypasses the 5-minute rate limit (escalations are always sent)

    Both functions must be wrapped in try/catch -- email failure is non-fatal (log warning, continue).
  </action>
  <verify>
    - `npx tsc --noEmit` passes with no type errors
    - All 3 files compile and export their documented functions
    - guardrails.ts: checkGuardrails returns { allowed: false } when kill switch preference is set to 'true'
    - runbooks.ts: findRunbook('VM_CRASHED') returns the vm-crashed-restart runbook
    - reporter.ts: sendRemediationEmail function exists and handles try/catch properly
  </verify>
  <done>
    - 3 runbooks defined (VM restart, CT restart, node WOL)
    - Guardrails enforce kill switch, rate limit (3/hr), blast radius (1 node), autonomy level
    - Email reporter sends HTML emails via agent1 SSH with 5-minute rate limiting
    - Kill switch double-check pattern implemented in executeRunbook
    - All errors caught -- no function can crash the process
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire runbooks into pollers and add monitor REST API</name>
  <files>
    src/monitor/poller.ts
    src/monitor/index.ts
    src/api/routes.ts
  </files>
  <action>
    **1. Update `src/monitor/poller.ts`** to call executeRunbook when state changes are detected:

    In `pollCritical()`, after detecting state changes via stateTracker:
    - For each StateChange, create an Incident object: `{ id: crypto.randomUUID(), key: \`${change.type}:${change.target}\`, type: change.type, node: change.node, target: change.target, detectedAt: new Date().toISOString(), details: { previousState: change.previousState, currentState: change.currentState } }`
    - Call `executeRunbook(incident, eventsNs)` -- do NOT await it (fire-and-forget so the poll loop continues). But DO handle the promise rejection: `.catch(err => console.error('[Monitor] Runbook error:', err))`

    **IMPORTANT:** Import executeRunbook from './runbooks.js'. Import crypto from 'node:crypto'.

    **2. Update `src/monitor/index.ts`** to export monitor status info:

    Add an exported function:
    - `getMonitorStatus(): { running: boolean; autonomyLevel: number; killSwitch: boolean; activeRemediations: number }` -- reads current state from guardrails module and returns aggregated status

    **3. Add monitor REST API endpoints to `src/api/routes.ts`**:

    Add these routes AFTER the existing auth middleware but before the tool routes:

    ```typescript
    // GET /api/monitor/status -- current monitor state
    router.get('/api/monitor/status', (_req: Request, res: Response) => {
      const killSwitch = memoryStore.getPreference('autonomy.killSwitch');
      const autonomyLevel = memoryStore.getPreference('autonomy.level');
      // Import getMonitorStatus if available, otherwise construct from preferences
      res.json({
        killSwitch: killSwitch?.value === 'true',
        autonomyLevel: parseInt(autonomyLevel?.value ?? '3', 10),
        running: true, // Monitor is always running when server is up
      });
    });

    // PUT /api/monitor/killswitch -- toggle autonomous actions
    router.put('/api/monitor/killswitch', (req: Request, res: Response) => {
      const { active } = req.body as { active: boolean };
      memoryStore.setPreference('autonomy.killSwitch', String(active));

      // Log the toggle as an event
      memoryStore.saveEvent({
        type: 'status',
        severity: active ? 'warning' : 'info',
        source: 'user',
        summary: active ? 'KILL SWITCH ACTIVATED -- autonomous actions disabled' : 'Kill switch deactivated -- autonomous actions re-enabled',
      });

      // Emit kill switch state change to all connected event clients
      // Need eventsNs -- import from index.ts
      const { eventsNs } = await import('../index.js');
      eventsNs.emit('event', {
        id: crypto.randomUUID(),
        type: 'status',
        severity: active ? 'warning' : 'info',
        title: active ? 'KILL SWITCH ACTIVATED' : 'Kill switch deactivated',
        message: active ? 'All autonomous actions disabled by operator' : 'Autonomous actions re-enabled',
        timestamp: new Date().toISOString(),
      });

      res.json({ killSwitch: active });
    });

    // GET /api/monitor/actions -- recent autonomy actions from audit log
    router.get('/api/monitor/actions', (req: Request, res: Response) => {
      const { limit } = req.query;
      const actions = memoryStore.getAutonomyActions(limit ? parseInt(limit as string, 10) : 50);
      res.json({ actions });
    });

    // PUT /api/monitor/autonomy-level -- set the autonomy level
    router.put('/api/monitor/autonomy-level', (req: Request, res: Response) => {
      const { level } = req.body as { level: number };
      if (typeof level !== 'number' || level < 0 || level > 4) {
        res.status(400).json({ error: 'Level must be 0-4' });
        return;
      }
      memoryStore.setPreference('autonomy.level', String(level));
      res.json({ autonomyLevel: level });
    });
    ```

    **NOTE on circular import:** The killswitch route needs eventsNs to emit events. Use dynamic import `const { eventsNs } = await import('../index.js')` inside the route handler to avoid circular dependency. The route handler is async so this works fine. Alternatively, accept eventsNs via a setter function that index.ts calls after socket setup. Use whichever approach avoids circular import issues at compile time -- the dynamic import is simpler.

    **NOTE on crypto:** Use `import { randomUUID } from 'node:crypto'` at the top of routes.ts for the kill switch event ID.
  </action>
  <verify>
    - `npx tsc --noEmit` passes with no type errors
    - Server starts and the monitor/runbook system is wired end-to-end
    - `curl -X GET http://localhost:4000/api/monitor/status` (with auth header) returns `{ killSwitch: false, autonomyLevel: 3, running: true }`
    - `curl -X PUT http://localhost:4000/api/monitor/killswitch -H 'Content-Type: application/json' -d '{"active":true}'` sets the kill switch
    - After setting kill switch, `GET /api/monitor/status` returns `{ killSwitch: true, ... }`
    - The /events namespace receives a "KILL SWITCH ACTIVATED" event
    - `curl -X GET http://localhost:4000/api/monitor/actions` returns `{ actions: [] }` (empty initially)
  </verify>
  <done>
    - Detected state changes trigger runbook execution pipeline
    - Monitor REST API exposes status, kill switch toggle, autonomy level, and action history
    - Kill switch changes emit real-time events to connected dashboard clients
    - Full remediation pipeline: detect -> match runbook -> check guardrails -> execute tool -> verify -> report -> log
    - Escalation after 3 failures triggers diagnostic email
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` -- zero type errors
2. End-to-end test: Set kill switch active, verify all autonomous actions are blocked
3. End-to-end test: Set kill switch inactive, simulate a detection (if a VM happens to transition), verify runbook executes
4. API test: GET /api/monitor/status returns correct state
5. API test: PUT /api/monitor/killswitch toggles the state and emits events
6. API test: GET /api/monitor/actions returns audit log entries
7. Rate limiter test: After 3 attempts for same incident key, 4th attempt is blocked
8. Blast radius test: While one remediation is active, a second is blocked
</verification>

<success_criteria>
- Runbooks execute via executeTool pipeline with full safety tier enforcement
- Kill switch immediately stops all autonomous actions when activated
- Rate limiting prevents more than 3 remediation attempts per issue per hour
- Blast radius control prevents simultaneous multi-node remediation
- Escalation emails are sent after 3 failed attempts
- Every autonomous action is recorded in the autonomy_actions audit log
- All errors are caught -- the monitor never crashes the backend
</success_criteria>

<output>
After completion, create `.planning/phases/04-autonomous-monitoring-remediation/04-02-SUMMARY.md`
</output>
