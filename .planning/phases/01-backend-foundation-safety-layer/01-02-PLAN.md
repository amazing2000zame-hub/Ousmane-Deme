---
phase: 01-backend-foundation-safety-layer
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - jarvis-backend/src/clients/proxmox.ts
  - jarvis-backend/src/clients/ssh.ts
  - jarvis-backend/src/config.ts
autonomous: true

must_haves:
  truths:
    - "Proxmox client fetches node list from all 4 cluster nodes via REST API"
    - "Proxmox client fetches VM/container status from /cluster/resources"
    - "Proxmox client can start/stop a VM via POST to the correct PVE endpoint"
    - "SSH client connects to any cluster node and executes a command"
    - "SSH client reuses connections (connection pooling per host)"
    - "All external calls have timeouts (10s connect, 30s command)"
    - "Self-signed TLS certificates are accepted without error"
  artifacts:
    - path: "jarvis-backend/src/clients/proxmox.ts"
      provides: "Proxmox REST API client with token auth"
      exports: ["ProxmoxClient", "proxmoxClients", "getAnyClient"]
      min_lines: 80
    - path: "jarvis-backend/src/clients/ssh.ts"
      provides: "SSH client with connection pooling"
      exports: ["getSSHConnection", "execOnNode", "closeAllConnections"]
      min_lines: 50
  key_links:
    - from: "jarvis-backend/src/clients/proxmox.ts"
      to: "Proxmox nodes HTTPS:8006"
      via: "fetch with PVEAPIToken auth header"
      pattern: "PVEAPIToken="
    - from: "jarvis-backend/src/clients/ssh.ts"
      to: "Cluster nodes SSH:22"
      via: "node-ssh with privateKey"
      pattern: "privateKey"
    - from: "jarvis-backend/src/clients/proxmox.ts"
      to: "jarvis-backend/src/config.ts"
      via: "imports CLUSTER_NODES, PVE_TOKEN_ID, PVE_TOKEN_SECRET"
      pattern: "config"
---

<objective>
Build the two infrastructure clients that connect Jarvis to the Proxmox cluster: a REST API client for Proxmox (HTTPS to port 8006 on each node with API token auth) and an SSH client with connection pooling (for shell commands on cluster nodes).

Purpose: These clients are the "hands" of the system. Every MCP tool in Plan 03 calls these clients to interact with the cluster. Without working clients, no tools function.

Output: Two TypeScript modules -- ProxmoxClient class for REST API operations and SSH client functions for command execution -- both tested against the real cluster.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-backend-foundation-safety-layer/01-RESEARCH.md

# Prior plan context (need project structure from 01-01)
# Will reference 01-01-SUMMARY.md once it exists for exact file layout
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Proxmox REST API client with token auth and all required methods</name>
  <files>
    jarvis-backend/src/clients/proxmox.ts
  </files>
  <action>
    Create the Proxmox REST API client following the pattern from 01-RESEARCH.md Code Example #2.

    **ProxmoxClient class:**
    - Constructor accepts: { host: string, port?: number, tokenId: string, tokenSecret: string }
    - Builds baseUrl as `https://${host}:${port || 8006}/api2/json`
    - Builds Authorization header as `PVEAPIToken=${tokenId}=${tokenSecret}`
    - Self-signed TLS handled via NODE_TLS_REJECT_UNAUTHORIZED=0 env var (set in Docker Compose, no per-request agent needed)

    **Generic methods:**
    - `async get<T>(path: string): Promise<T>` -- GET request, unwrap {data: T} envelope, throw on non-OK response with status and path info
    - `async post<T>(path: string, body?: Record<string, unknown>): Promise<T>` -- POST with JSON body, unwrap {data: T}

    **Domain methods (all delegate to get/post):**
    - `getNodes()` -- GET /nodes
    - `getNodeStatus(node: string)` -- GET /nodes/{node}/status
    - `getClusterResources(type?: string)` -- GET /cluster/resources (optional ?type= query param)
    - `getClusterStatus()` -- GET /cluster/status
    - `getNodeStorage(node: string)` -- GET /nodes/{node}/storage
    - `getRecentTasks(limit?: number)` -- GET /cluster/tasks?limit={limit || 50}
    - `startVM(node: string, vmid: number)` -- POST /nodes/{node}/qemu/{vmid}/status/start
    - `stopVM(node: string, vmid: number)` -- POST /nodes/{node}/qemu/{vmid}/status/stop
    - `rebootVM(node: string, vmid: number)` -- POST /nodes/{node}/qemu/{vmid}/status/reboot
    - `shutdownVM(node: string, vmid: number)` -- POST /nodes/{node}/qemu/{vmid}/status/shutdown
    - `startCT(node: string, vmid: number)` -- POST /nodes/{node}/lxc/{vmid}/status/start
    - `stopCT(node: string, vmid: number)` -- POST /nodes/{node}/lxc/{vmid}/status/stop
    - `rebootCT(node: string, vmid: number)` -- POST /nodes/{node}/lxc/{vmid}/status/reboot

    **Request timeout:** Set 15-second timeout on all fetch calls using AbortController:
    ```typescript
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort(), 15000);
    try {
      const response = await fetch(url, { ...options, signal: controller.signal });
      // ...
    } finally {
      clearTimeout(timeout);
    }
    ```

    **Error handling:** Wrap all fetch calls in try/catch. On network error or timeout, throw with descriptive message including the host and path. On HTTP error, include status code and response text.

    **Client instances:**
    - Export `proxmoxClients`: Map<string, ProxmoxClient> initialized from config.CLUSTER_NODES
    - Export `getAnyClient()`: returns any client (use first node, typically Home/192.168.1.50) for cluster-wide queries
    - Export `getClientForNode(nodeName: string)`: returns client for specific node

    The client map creates one ProxmoxClient per cluster node, all sharing the same token ID and secret (Proxmox cluster shares user DB).

    IMPORTANT: The Proxmox API wraps all responses in a `{data: ...}` envelope. The get/post methods MUST unwrap this. Callers should receive the inner data directly.

    IMPORTANT: API tokens do NOT need CSRF tokens for POST/PUT/DELETE (unlike ticket auth). Do not add CSRFPreventionToken header.
  </action>
  <verify>
    1. `cd /root/jarvis-backend && npx tsc --noEmit` -- compiles without errors

    2. Quick runtime test (requires Proxmox API token to exist -- if not yet created, verify with a basic connectivity test):
    ```bash
    cd /root/jarvis-backend && npx tsx -e "
      import { ProxmoxClient } from './src/clients/proxmox.js';
      const c = new ProxmoxClient({
        host: '192.168.1.50',
        tokenId: process.env.PVE_TOKEN_ID || 'root@pam!jarvis',
        tokenSecret: process.env.PVE_TOKEN_SECRET || 'test'
      });
      console.log('Client created, baseUrl:', c['baseUrl']);
    "
    ```
    Should print the baseUrl without errors.

    If PVE_TOKEN_SECRET is set correctly (token created on nodes), also test:
    ```bash
    cd /root/jarvis-backend && npx tsx -e "
      import { getAnyClient } from './src/clients/proxmox.js';
      const client = getAnyClient();
      const nodes = await client.getNodes();
      console.log('Nodes:', JSON.stringify(nodes, null, 2));
    "
    ```
    Should return array of 4 nodes.
  </verify>
  <done>
    ProxmoxClient class connects to Proxmox REST API with token auth, has get/post generic methods with 15s timeout, all domain methods (getNodes, getNodeStatus, getClusterResources, getClusterStatus, getNodeStorage, getRecentTasks, startVM, stopVM, rebootVM, shutdownVM, startCT, stopCT, rebootCT), proper error handling, and client instances created for all 4 cluster nodes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create SSH client with connection pooling and command execution</name>
  <files>
    jarvis-backend/src/clients/ssh.ts
    jarvis-backend/src/config.ts
  </files>
  <action>
    Create the SSH client following the pattern from 01-RESEARCH.md Code Example #8.

    **Connection pool:**
    - Maintain a Map<string, NodeSSH> of persistent connections (one per host)
    - `getSSHConnection(host: string)`: Check if existing connection is alive (isConnected()). If alive, return it. If dead or missing, create new connection.
    - Connect with: host, username 'root', privateKey path from config.SSH_KEY_PATH, readyTimeout 10000 (10s)
    - On connection failure, remove stale entry from pool and throw descriptive error

    **Command execution:**
    - `execOnNode(host: string, command: string, timeout?: number)`: Get pooled connection, execute command via ssh.execCommand()
    - Default timeout: 30000 (30s)
    - Return: { stdout: string, stderr: string, code: number | null }
    - On timeout or error, return descriptive error (do NOT crash the process)

    **Host resolution:**
    - `execOnNodeByName(nodeName: string, command: string, timeout?: number)`: Resolve node name (Home, pve, agent1, agent) to IP using config.CLUSTER_NODES, then call execOnNode
    - Throw if node name is not recognized

    **Cleanup:**
    - `closeAllConnections()`: Dispose all pooled connections, clear the map
    - Call this from the graceful shutdown handler in index.ts (update config.ts or index.ts to wire this)

    **Update src/config.ts** if needed to ensure CLUSTER_NODES includes both name and host IP.

    IMPORTANT: node-ssh accepts `privateKey` as a file path string -- it reads the file itself. Do NOT manually read the key file.

    IMPORTANT: Wrap all SSH operations in try/catch. A failed SSH command must never crash the server. Return the error in the result object.
  </action>
  <verify>
    1. `cd /root/jarvis-backend && npx tsc --noEmit` -- compiles without errors

    2. Test SSH connection to local node (Home, 192.168.1.50):
    ```bash
    cd /root/jarvis-backend && npx tsx -e "
      import { execOnNodeByName } from './src/clients/ssh.js';
      const result = await execOnNodeByName('Home', 'hostname');
      console.log('Result:', JSON.stringify(result));
    "
    ```
    Should return { stdout: 'Home', stderr: '', code: 0 } (or the actual hostname).

    3. Test SSH to another node:
    ```bash
    cd /root/jarvis-backend && npx tsx -e "
      import { execOnNodeByName } from './src/clients/ssh.js';
      const result = await execOnNodeByName('pve', 'uptime');
      console.log('Result:', JSON.stringify(result));
    "
    ```
    Should return uptime info.

    4. Test connection pooling (second call should reuse connection):
    ```bash
    cd /root/jarvis-backend && npx tsx -e "
      import { execOnNodeByName, closeAllConnections } from './src/clients/ssh.js';
      console.time('first');
      await execOnNodeByName('Home', 'hostname');
      console.timeEnd('first');
      console.time('second');
      await execOnNodeByName('Home', 'hostname');
      console.timeEnd('second');
      await closeAllConnections();
    "
    ```
    Second call should be noticeably faster (no SSH handshake).
  </verify>
  <done>
    SSH client connects to all 4 cluster nodes using key-based auth, reuses connections via pooling (Map<host, NodeSSH>), executes commands with 30s default timeout, resolves node names to IPs, and gracefully handles errors without crashing. Both execOnNode (by IP) and execOnNodeByName (by name) work. closeAllConnections cleans up on shutdown.
  </done>
</task>

</tasks>

<verification>
1. `cd /root/jarvis-backend && npx tsc --noEmit` -- all files compile
2. ProxmoxClient creates instances for all 4 nodes from config
3. SSH client connects to Home (192.168.1.50) and executes `hostname` successfully
4. SSH client connects to pve (192.168.1.74) and executes `uptime` successfully
5. SSH connection pooling works (second call faster than first)
6. All fetch calls have 15s timeout, all SSH commands have 30s timeout
</verification>

<success_criteria>
- ProxmoxClient connects to Proxmox REST API with token auth on all 4 nodes
- ProxmoxClient has all domain methods (get/post, node status, cluster resources, VM/CT lifecycle)
- SSH client executes commands on cluster nodes via key-based auth
- Connection pooling reuses SSH connections
- All external calls have timeouts
- Zero TypeScript compilation errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-backend-foundation-safety-layer/01-02-SUMMARY.md`
</output>
