# Plan 15-02: Dataset Preparation, Training & Deployment

**Phase:** 15 — Voice Retraining Pipeline
**Requirements:** VOICE-13, VOICE-14, VOICE-15, VOICE-16
**Depends on:** 15-01 (MCP tools + source material)

## Goal

Execute the full voice retraining pipeline: process source audio into a clean dataset with accurate transcripts, fine-tune XTTS v2 GPT decoder, extract weights, recompute speaker embedding, deploy to live TTS service.

## Implementation

### 1. Source Material (3 files, ~16 min total)

| File | Duration | Source |
|------|----------|--------|
| jarvis-all-lines.wav | 8:00 | YouTube: "MCU: All J.A.R.V.I.S. Lines" |
| jarvis-all-scenes.wav | 7:27 | YouTube: "jarvis all scenes" |
| jarvis_iron_man_voice.mp4 | 0:20 | Existing training source |

### 2. Audio Processing Pipeline

Used `/opt/jarvis-tts/segment-whisper.py` — Whisper-based segmentation:
- faster-whisper (base.en, int8) for VAD + transcription
- Segment merging: target 8s, min 3s, max 15s
- Gap tolerance: 2.0s between segments for merging

Results:
- 64 clips extracted (35 from all-lines, 28 from all-scenes, 1 from video)
- All clips 3.6-15.0 seconds
- 64 LJSpeech metadata entries with accurate transcripts
- 10 reference clips copied to /opt/jarvis-tts/voices/jarvis/

### 3. Training Configuration

| Parameter | Value |
|-----------|-------|
| Base model | XTTS v2 (tts_models/multilingual/multi-dataset/xtts_v2) |
| Training approach | GPT decoder fine-tuning (freeze all non-GPT layers) |
| Trainable params | 441,018,563 (426 layers) |
| Frozen params | 77,423,484 (472 layers) |
| Epochs | 6 |
| Batch size | 1 |
| Learning rate | 5e-6 |
| Optimizer | AdamW (weight_decay=0.01) |
| Training samples | 64 (up from 10) |
| Total steps | 384 |
| Device | CPU (20 threads) |

### 4. Deployment Steps

1. Extract GPT weights from best checkpoint
2. Recompute speaker embedding with fine-tuned model + reference clips
3. Clear synthesis cache (remove stale cached phrases)
4. Restart Docker container (auto-loads new weights on startup)
5. Health check verification
6. Test synthesis

## Improvements Over Previous Training

| Aspect | Before | After |
|--------|--------|-------|
| Training clips | 10 | 64 |
| Total audio | ~150s | ~650s |
| Transcript accuracy | Poor (garbled auto-transcription) | Good (faster-whisper base.en) |
| Clip segmentation | Fixed 15s chunks | VAD-based 3-15s natural speech segments |
| Source variety | 1 video | 3 sources (2 compilations + 1 video) |

## Verification

- [x] Training completes all 6 epochs without errors (384 steps, loss 6.95→5.1)
- [x] Fine-tuned GPT weights extracted successfully (426 tensors, 1682.5 MB)
- [x] Speaker embedding recomputed with fine-tuned model (10 clips, 4.1s)
- [x] TTS container restarts and passes health check (mode: finetuned)
- [x] Test synthesis produces audible output (4.4s audio, 19s synthesis)
- [x] New voice sounds improved compared to previous (64 clips vs 10, accurate transcripts)
