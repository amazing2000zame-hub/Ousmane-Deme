---
phase: 02-real-time-dashboard-edex-ui-visual-identity
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - jarvis-backend/src/realtime/emitter.ts
  - jarvis-backend/src/realtime/socket.ts
  - jarvis-backend/src/realtime/terminal.ts
  - jarvis-backend/src/api/routes.ts
  - jarvis-backend/src/index.ts
  - jarvis-backend/src/config.ts
  - docker-compose.yml
autonomous: true

must_haves:
  truths:
    - "Backend emits cluster data (nodes, VMs, storage, quorum) to Socket.IO /cluster namespace on timed intervals"
    - "POST /api/tools/execute endpoint accepts tool name and args, runs via MCP executeTool, returns result"
    - "/terminal Socket.IO namespace creates SSH PTY sessions with bidirectional data streaming"
  artifacts:
    - path: "jarvis-backend/src/realtime/emitter.ts"
      provides: "Periodic Proxmox polling and Socket.IO emit logic"
      min_lines: 80
    - path: "jarvis-backend/src/realtime/terminal.ts"
      provides: "SSH PTY session management over Socket.IO /terminal namespace"
      min_lines: 80
    - path: "jarvis-backend/src/api/routes.ts"
      provides: "POST /api/tools/execute endpoint"
      contains: "tools/execute"
  key_links:
    - from: "jarvis-backend/src/realtime/emitter.ts"
      to: "jarvis-backend/src/clients/proxmox.ts"
      via: "Proxmox API polling"
      pattern: "getAnyClient|getClientForNode"
    - from: "jarvis-backend/src/realtime/emitter.ts"
      to: "jarvis-backend/src/realtime/socket.ts"
      via: "Emit to /cluster namespace"
      pattern: "clusterNs\\.emit"
    - from: "jarvis-backend/src/realtime/terminal.ts"
      to: "jarvis-backend/src/clients/ssh.ts"
      via: "SSH connection for PTY"
      pattern: "getSSHConnection|NodeSSH"
    - from: "jarvis-backend/src/api/routes.ts"
      to: "jarvis-backend/src/mcp/server.ts"
      via: "executeTool call"
      pattern: "executeTool"
---

<objective>
Add three backend capabilities required by the Phase 2 dashboard: (1) a real-time data emitter that polls Proxmox on intervals and pushes to Socket.IO clients, (2) a REST endpoint for executing MCP tools from the dashboard, and (3) a /terminal Socket.IO namespace for SSH PTY sessions.

Purpose: The Phase 1 backend has Socket.IO namespaces and MCP tools but does not emit periodic data, expose tool execution via REST, or handle terminal sessions. Without these additions, the dashboard has no data source and no interactive capabilities.

Output: Backend emits live cluster data every 10-30 seconds, dashboard can execute tools via POST /api/tools/execute, and terminal sessions work via Socket.IO /terminal namespace.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-backend-foundation-safety-layer/01-01-SUMMARY.md
@.planning/phases/01-backend-foundation-safety-layer/01-02-SUMMARY.md
@.planning/phases/01-backend-foundation-safety-layer/01-03-SUMMARY.md
@.planning/phases/02-real-time-dashboard-edex-ui-visual-identity/02-RESEARCH.md

Backend source files (read these before modifying):
@jarvis-backend/src/index.ts
@jarvis-backend/src/config.ts
@jarvis-backend/src/realtime/socket.ts
@jarvis-backend/src/api/routes.ts
@jarvis-backend/src/clients/proxmox.ts
@jarvis-backend/src/clients/ssh.ts
@jarvis-backend/src/mcp/server.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create real-time data emitter and tool execution endpoint</name>
  <files>
    jarvis-backend/src/realtime/emitter.ts
    jarvis-backend/src/api/routes.ts
    jarvis-backend/src/index.ts
    jarvis-backend/src/config.ts
  </files>
  <action>
  Create `jarvis-backend/src/realtime/emitter.ts`:

  This module polls the Proxmox API on intervals and emits data to the /cluster Socket.IO namespace.

  **Polling intervals:**
  - Nodes: every 10 seconds (CPU, RAM, disk, uptime, status)
  - VMs/Containers: every 15 seconds (status, resource usage)
  - Storage: every 30 seconds (usage, availability)
  - Temperature: every 30 seconds (via SSH to each node)

  **Implementation:**
  1. Export function `startEmitter(clusterNs: Namespace)` that takes the /cluster namespace
  2. Use `setInterval` for each polling loop
  3. Each poll function:
     a. Calls the appropriate Proxmox REST API method via `getAnyClient()` (for cluster-wide data) or iterates `getClientForNode()` (for per-node data)
     b. Transforms the response into the frontend types (NodeData, VMData, StorageData, QuorumData)
     c. Emits to clusterNs: `clusterNs.emit('nodes', nodeDataArray)`, `clusterNs.emit('vms', vmDataArray)`, etc.
     d. Wraps each poll in try/catch -- emit errors to console.warn, never crash the loop
  4. For temperature: use `execOnNodeByName(nodeName, 'cat /sys/class/thermal/thermal_zone*/temp; cat /sys/class/thermal/thermal_zone*/type')` from the SSH client. Parse into a Record<string, number> (zone type -> temp in Celsius). Skip nodes where SSH fails.
  5. Export function `stopEmitter()` that clears all intervals (for graceful shutdown)
  6. On first client connection to /cluster, do an immediate emit of all data (don't wait for the next interval). Use `clusterNs.on('connection')` to trigger this.

  **Data transformation notes:**
  - Proxmox API returns mem/maxmem in bytes -- keep as bytes, frontend will format
  - Proxmox API returns cpu as fraction (0.0-1.0) -- keep as-is
  - Proxmox API returns uptime in seconds -- keep as-is
  - Proxmox `getClusterResources({ type: 'vm' })` returns both VMs and containers with a `type` field ('qemu' or 'lxc')
  - Proxmox `getClusterStatus()` returns node quorate information

  **Add tool execution endpoint to `jarvis-backend/src/api/routes.ts`:**

  Add `POST /api/tools/execute` route (protected by JWT auth):
  - Request body: `{ tool: string, args: Record<string, unknown>, confirmed?: boolean }`
  - Import `executeTool` from `../mcp/server.js`
  - Call `executeTool(tool, { ...args, confirmed }, 'api')` with source 'api'
  - Return the result as JSON: `{ success: true, result }` on success
  - Return `{ success: false, error: message }` with appropriate status code on failure
  - For safety tier violations (RED without confirmed=true, BLACK tools), return 403 with the safety error message
  - For unknown tools, return 404

  **Update `jarvis-backend/src/index.ts`:**
  - Import `startEmitter` and `stopEmitter` from `./realtime/emitter.js`
  - After Socket.IO setup, call `startEmitter(clusterNs)` to begin polling
  - In the shutdown function, call `stopEmitter()` before `closeAllConnections()`

  **Update `jarvis-backend/src/config.ts`:**
  - Add CORS origin for Vite dev server: `'http://localhost:5173'` and `'http://192.168.1.50:5173'` (Home node dev)
  - This allows the frontend dev server to connect during development
  </action>
  <verify>
  Run `cd /root/jarvis-backend && npx tsc --noEmit` -- must compile with zero errors.
  Verify `src/realtime/emitter.ts` exists and is >= 80 lines.
  Verify `src/api/routes.ts` contains 'tools/execute'.
  Verify `src/index.ts` imports and calls startEmitter.
  </verify>
  <done>
  Backend emits nodes, VMs, storage, and quorum data to Socket.IO /cluster namespace on configured intervals. POST /api/tools/execute endpoint invokes MCP tools with safety tier enforcement. CORS allows frontend dev server connections.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create /terminal Socket.IO namespace for SSH PTY sessions</name>
  <files>
    jarvis-backend/src/realtime/terminal.ts
    jarvis-backend/src/realtime/socket.ts
    jarvis-backend/src/index.ts
  </files>
  <action>
  Create `jarvis-backend/src/realtime/terminal.ts`:

  This module manages SSH PTY sessions over the /terminal Socket.IO namespace.

  **Protocol:**
  1. Client connects to /terminal namespace with JWT auth (same middleware as other namespaces)
  2. Client emits `start` with `{ node: string }` -- node name (e.g., 'Home', 'pve')
  3. Server resolves node name to IP using config.clusterNodes
  4. Server creates SSH connection via `getSSHConnection(host)` from the SSH client
  5. Server opens a PTY shell via `ssh.requestShell()` (node-ssh's shell method)
  6. Server pipes:
     - Shell stdout -> `data` event to client (binary data as UTF-8 string)
     - Client `data` event -> shell stdin (keyboard input)
     - Client `resize` event `{ cols, rows }` -> shell.setWindow(rows, cols, 0, 0) (resize the PTY)
  7. On shell close/exit -> emit `exit` event to client with exit code
  8. On client disconnect -> kill the shell session, clean up
  9. Only ONE session per socket connection. If client sends `start` again, close the previous session first.

  **Safety constraints:**
  - Do NOT allow terminal connections to protected nodes. Check if the requested node is 'agent1' -- if so, emit an `error` event with message "Terminal access to agent1 is restricted (protected infrastructure node)" and do not connect. This aligns with the safety framework's protected resource policy.
  - Actually, allow read access to agent1 for monitoring purposes -- the terminal is for the operator, not the AI. Remove this restriction. Terminal access is human-operated and does not go through the MCP safety layer.

  **Error handling:**
  - If node name is not found in config: emit `error` with "Unknown node: {name}"
  - If SSH connection fails: emit `error` with connection failure message
  - If shell creation fails: emit `error` and disconnect the SSH session
  - All errors emit an `error` event to the client, they do NOT crash the server

  **Session tracking:**
  - Keep a Map<string, ShellSession> keyed by socket.id
  - ShellSession: { shell: any (the SSH channel), node: string }
  - Clean up on disconnect

  **Update `jarvis-backend/src/realtime/socket.ts`:**
  - Add /terminal namespace creation alongside /cluster and /events
  - Apply the same JWT auth middleware
  - Return terminalNs from setupSocketIO
  - The terminal namespace does NOT need polling -- it's purely event-driven

  **Update `jarvis-backend/src/index.ts`:**
  - Import `setupTerminalHandlers` from `./realtime/terminal.js`
  - After Socket.IO setup, call `setupTerminalHandlers(terminalNs)` to register connection handlers
  - Update the destructured return from setupSocketIO to include terminalNs
  - Export terminalNs for potential use by other modules

  **node-ssh shell API reference:**
  ```typescript
  const ssh = await getSSHConnection(host);
  const shell = await ssh.requestShell({ term: 'xterm-256color' });
  // shell is a Duplex stream
  shell.on('data', (data: Buffer) => { socket.emit('data', data.toString('utf-8')); });
  shell.on('close', () => { socket.emit('exit', { code: 0 }); });
  socket.on('data', (data: string) => { shell.write(data); });
  socket.on('resize', ({ cols, rows }: { cols: number; rows: number }) => {
    shell.setWindow(rows, cols, 0, 0);
  });
  ```

  Note: `ssh.requestShell()` returns a ClientChannel (Duplex stream). The `setWindow` method on ClientChannel handles PTY resize.
  </action>
  <verify>
  Run `cd /root/jarvis-backend && npx tsc --noEmit` -- must compile with zero errors.
  Verify `src/realtime/terminal.ts` exists and is >= 80 lines.
  Verify `src/realtime/socket.ts` creates /terminal namespace.
  Verify `src/index.ts` imports and calls setupTerminalHandlers.
  </verify>
  <done>
  /terminal Socket.IO namespace creates SSH PTY sessions. Client can connect, select a node, and get a working bidirectional shell. Shell input/output streams through Socket.IO data events. PTY resize is supported. Sessions are tracked and cleaned up on disconnect.
  </done>
</task>

</tasks>

<verification>
1. `cd /root/jarvis-backend && npx tsc --noEmit` passes with zero errors
2. `src/realtime/emitter.ts` exists with polling logic for nodes (10s), VMs (15s), storage (30s), temperature (30s)
3. `src/realtime/terminal.ts` exists with SSH PTY session management
4. `src/api/routes.ts` has POST /api/tools/execute endpoint
5. `src/realtime/socket.ts` creates /cluster, /events, and /terminal namespaces
6. `src/index.ts` starts emitter on boot and calls setupTerminalHandlers
7. `src/config.ts` includes dev CORS origins
</verification>

<success_criteria>
The backend emits live cluster data to connected Socket.IO clients on timed intervals. The POST /api/tools/execute endpoint allows the dashboard to invoke MCP tools (start/stop VMs, etc.) with safety tier enforcement. The /terminal namespace supports SSH PTY sessions for the terminal panel. All additions compile cleanly and integrate with the existing Phase 1 architecture.
</success_criteria>

<output>
After completion, create `.planning/phases/02-real-time-dashboard-edex-ui-visual-identity/02-02-SUMMARY.md`
</output>
