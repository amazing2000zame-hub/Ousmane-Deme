---
phase: 02-real-time-dashboard-edex-ui-visual-identity
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - jarvis-backend/src/realtime/emitter.ts
  - jarvis-backend/src/realtime/socket.ts
  - jarvis-backend/src/realtime/terminal.ts
  - jarvis-backend/src/api/routes.ts
  - jarvis-backend/src/index.ts
  - jarvis-backend/src/config.ts
  - docker-compose.yml
autonomous: true

must_haves:
  truths:
    - "Backend emits cluster data (nodes, VMs, storage, quorum) to Socket.IO /cluster namespace on timed intervals"
    - "POST /api/tools/execute endpoint accepts tool name and args, runs via MCP executeTool, returns result, and immediately emits updated resource data to /cluster namespace"
    - "/terminal Socket.IO namespace creates SSH PTY sessions with bidirectional data streaming using the SSH connection pool"
  artifacts:
    - path: "jarvis-backend/src/realtime/emitter.ts"
      provides: "Periodic Proxmox polling and Socket.IO emit logic, plus on-demand emit after tool execution"
      min_lines: 80
    - path: "jarvis-backend/src/realtime/terminal.ts"
      provides: "SSH PTY session management over Socket.IO /terminal namespace"
      min_lines: 80
    - path: "jarvis-backend/src/api/routes.ts"
      provides: "POST /api/tools/execute endpoint with immediate emit"
      contains: "tools/execute"
  key_links:
    - from: "jarvis-backend/src/realtime/emitter.ts"
      to: "jarvis-backend/src/clients/proxmox.ts"
      via: "Proxmox API polling"
      pattern: "getAnyClient|getClientForNode"
    - from: "jarvis-backend/src/realtime/emitter.ts"
      to: "jarvis-backend/src/realtime/socket.ts"
      via: "Emit to /cluster namespace"
      pattern: "clusterNs\\.emit"
    - from: "jarvis-backend/src/realtime/terminal.ts"
      to: "jarvis-backend/src/clients/ssh.ts"
      via: "SSH connection pool for PTY"
      pattern: "getSSHConnection"
    - from: "jarvis-backend/src/api/routes.ts"
      to: "jarvis-backend/src/mcp/server.ts"
      via: "executeTool call"
      pattern: "executeTool"
    - from: "jarvis-backend/src/api/routes.ts"
      to: "jarvis-backend/src/realtime/emitter.ts"
      via: "Immediate emit after tool execution"
      pattern: "emitNow|emitImmediate|refreshAndEmit"
---

<objective>
Add three backend capabilities required by the Phase 2 dashboard: (1) a real-time data emitter that polls Proxmox on intervals and pushes to Socket.IO clients, (2) a REST endpoint for executing MCP tools from the dashboard, and (3) a /terminal Socket.IO namespace for SSH PTY sessions.

Purpose: The Phase 1 backend has Socket.IO namespaces and MCP tools but does not emit periodic data, expose tool execution via REST, or handle terminal sessions. Without these additions, the dashboard has no data source and no interactive capabilities.

Output: Backend emits live cluster data every 10-30 seconds, dashboard can execute tools via POST /api/tools/execute (with immediate data refresh to clients), and terminal sessions work via Socket.IO /terminal namespace.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-backend-foundation-safety-layer/01-01-SUMMARY.md
@.planning/phases/01-backend-foundation-safety-layer/01-02-SUMMARY.md
@.planning/phases/01-backend-foundation-safety-layer/01-03-SUMMARY.md
@.planning/phases/02-real-time-dashboard-edex-ui-visual-identity/02-RESEARCH.md

Backend source files (read these before modifying):
@jarvis-backend/src/index.ts
@jarvis-backend/src/config.ts
@jarvis-backend/src/realtime/socket.ts
@jarvis-backend/src/api/routes.ts
@jarvis-backend/src/clients/proxmox.ts
@jarvis-backend/src/clients/ssh.ts
@jarvis-backend/src/mcp/server.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create real-time data emitter and tool execution endpoint</name>
  <files>
    jarvis-backend/src/realtime/emitter.ts
    jarvis-backend/src/api/routes.ts
    jarvis-backend/src/index.ts
    jarvis-backend/src/config.ts
  </files>
  <action>
  Create `jarvis-backend/src/realtime/emitter.ts`:

  This module polls the Proxmox API on intervals and emits data to the /cluster Socket.IO namespace.

  **Polling intervals:**
  - Nodes: every 10 seconds (CPU, RAM, disk, uptime, status)
  - VMs/Containers: every 15 seconds (status, resource usage)
  - Storage: every 30 seconds (usage, availability)
  - Temperature: every 30 seconds (via SSH to each node)

  **Implementation:**
  1. Export function `startEmitter(clusterNs: Namespace)` that takes the /cluster namespace
  2. Use `setInterval` for each polling loop
  3. Each poll function:
     a. Calls the appropriate Proxmox REST API method via `getAnyClient()` (for cluster-wide data) or iterates `getClientForNode()` (for per-node data)
     b. Transforms the response into the frontend types (NodeData, VMData, StorageData, QuorumData)
     c. Emits to clusterNs: `clusterNs.emit('nodes', nodeDataArray)`, `clusterNs.emit('vms', vmDataArray)`, etc.
     d. Wraps each poll in try/catch -- emit errors to console.warn, never crash the loop
  4. For temperature: use `execOnNodeByName(nodeName, 'cat /sys/class/thermal/thermal_zone*/temp; cat /sys/class/thermal/thermal_zone*/type')` from the SSH client. Parse into a Record<string, number> (zone type -> temp in Celsius). Skip nodes where SSH fails.
  5. Export function `stopEmitter()` that clears all intervals (for graceful shutdown)
  6. On first client connection to /cluster, do an immediate emit of all data (don't wait for the next interval). Use `clusterNs.on('connection')` to trigger this.

  **On-demand emit functions (for use by the tool execution endpoint):**
  7. Export function `emitNodesNow()` that immediately polls and emits nodes + VMs data to the /cluster namespace. This is called after tool execution so the user sees the result without waiting up to 15s.
  8. Export function `emitStorageNow()` that immediately polls and emits storage data.
  9. These reuse the same poll+transform+emit logic from the interval functions, just called on-demand.

  **Data transformation notes:**
  - Proxmox API returns mem/maxmem in bytes -- keep as bytes, frontend will format
  - Proxmox API returns cpu as fraction (0.0-1.0) -- keep as-is
  - Proxmox API returns uptime in seconds -- keep as-is
  - Proxmox `getClusterResources({ type: 'vm' })` returns both VMs and containers with a `type` field ('qemu' or 'lxc')
  - Proxmox `getClusterStatus()` returns node quorate information

  **Add tool execution endpoint to `jarvis-backend/src/api/routes.ts`:**

  Add `POST /api/tools/execute` route (protected by JWT auth):
  - Request body: `{ tool: string, args: Record<string, unknown>, confirmed?: boolean }`
  - Import `executeTool` from `../mcp/server.js`
  - Import `emitNodesNow` and `emitStorageNow` from `../realtime/emitter.js`
  - Call `executeTool(tool, { ...args, confirmed }, 'api')` with source 'api'
  - Return the result as JSON: `{ success: true, result }` on success
  - Return `{ success: false, error: message }` with appropriate status code on failure
  - For safety tier violations (RED without confirmed=true, BLACK tools), return 403 with the safety error message
  - For unknown tools, return 404
  - **CRITICAL: After successful tool execution, immediately trigger a data refresh to connected clients.** Determine the affected resource type from the tool name:
    - VM/container tools (start_vm, stop_vm, restart_vm, start_container, stop_container, restart_container, etc.): call `await emitNodesNow()` which emits both nodes and VMs data
    - Storage-related tools: call `await emitStorageNow()`
    - For other tools: call `await emitNodesNow()` as a safe default
  - This emit happens BEFORE returning the API response, so the WebSocket clients receive updated data within milliseconds of the action completing.

  **Update `jarvis-backend/src/index.ts`:**
  - Import `startEmitter` and `stopEmitter` from `./realtime/emitter.js`
  - After Socket.IO setup, call `startEmitter(clusterNs)` to begin polling
  - In the shutdown function, call `stopEmitter()` before `closeAllConnections()`

  **Update `jarvis-backend/src/config.ts`:**
  - Add CORS origin for Vite dev server: `'http://localhost:5173'` and `'http://192.168.1.50:5173'` (Home node dev)
  - This allows the frontend dev server to connect during development
  </action>
  <verify>
  Run `cd /root/jarvis-backend && npx tsc --noEmit` -- must compile with zero errors.
  Verify `src/realtime/emitter.ts` exists and is >= 80 lines.
  Verify `src/realtime/emitter.ts` exports `emitNodesNow` and `emitStorageNow` functions.
  Verify `src/api/routes.ts` contains 'tools/execute'.
  Verify `src/api/routes.ts` imports and calls `emitNodesNow` after tool execution.
  Verify `src/index.ts` imports and calls startEmitter.
  </verify>
  <done>
  Backend emits nodes, VMs, storage, and quorum data to Socket.IO /cluster namespace on configured intervals. POST /api/tools/execute endpoint invokes MCP tools with safety tier enforcement and immediately emits updated resource data to all connected clients (no 15s wait for UI to reflect changes). CORS allows frontend dev server connections.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create /terminal Socket.IO namespace for SSH PTY sessions</name>
  <files>
    jarvis-backend/src/realtime/terminal.ts
    jarvis-backend/src/realtime/socket.ts
    jarvis-backend/src/index.ts
  </files>
  <action>
  Create `jarvis-backend/src/realtime/terminal.ts`:

  This module manages SSH PTY sessions over the /terminal Socket.IO namespace.

  **Protocol:**
  1. Client connects to /terminal namespace with JWT auth (same middleware as other namespaces)
  2. Client emits `start` with `{ node: string }` -- node name (e.g., 'Home', 'pve')
  3. Server resolves node name to IP using config.clusterNodes
  4. Server creates SSH connection via `getSSHConnection(host)` from the SSH client (`jarvis-backend/src/clients/ssh.ts`). This MUST use the existing SSH connection pool to leverage connection pooling and auto-reconnect. Do NOT create a raw SSH connection outside the pool.
  5. Server opens a PTY shell via `ssh.requestShell()` (node-ssh's shell method)
  6. Server pipes:
     - Shell stdout -> `data` event to client (binary data as UTF-8 string)
     - Client `data` event -> shell stdin (keyboard input)
     - Client `resize` event `{ cols, rows }` -> shell.setWindow(rows, cols, 0, 0) (resize the PTY)
  7. On shell close/exit -> emit `exit` event to client with exit code
  8. On client disconnect -> kill the shell session, clean up the SSH connection properly (see cleanup section below)
  9. Only ONE session per socket connection. If client sends `start` again, close the previous session first.

  **SSH connection cleanup on disconnect:**
  - When a socket disconnects, the shell channel MUST be closed via `shell.close()` or `shell.end()`.
  - The SSH connection obtained via `getSSHConnection(host)` is pooled. Do NOT call `ssh.dispose()` directly -- that would destroy the pooled connection for other users.
  - Instead, just close the shell channel. The pooled SSH connection remains available for reuse by other terminal sessions or the emitter.
  - If the SSH connection itself dies (e.g., node goes offline), the pool handles reconnection on next use. Just clean up the shell reference and update session state.
  - Track cleanup state to avoid double-close: set a `cleaned` flag in the session, check it before cleanup runs.

  **Safety constraints:**
  - Terminal access is human-operated and does not go through the MCP safety layer. All nodes are accessible (including agent1) since this is the operator's direct access, not AI-driven.

  **Error handling:**
  - If node name is not found in config: emit `error` with "Unknown node: {name}"
  - If SSH connection fails: emit `error` with connection failure message
  - If shell creation fails: emit `error` and do NOT dispose the pooled SSH connection -- just report the error
  - All errors emit an `error` event to the client, they do NOT crash the server

  **Session tracking:**
  - Keep a Map<string, ShellSession> keyed by socket.id
  - ShellSession: { shell: any (the SSH channel), node: string, cleaned: boolean }
  - On disconnect: iterate sessions for this socket.id, close shell channels, delete from map
  - On new `start` while session exists: close previous shell channel first, then create new session

  **Update `jarvis-backend/src/realtime/socket.ts`:**
  - Add /terminal namespace creation alongside /cluster and /events
  - Apply the same JWT auth middleware
  - Return terminalNs from setupSocketIO
  - The terminal namespace does NOT need polling -- it's purely event-driven

  **Update `jarvis-backend/src/index.ts`:**
  - Import `setupTerminalHandlers` from `./realtime/terminal.js`
  - After Socket.IO setup, call `setupTerminalHandlers(terminalNs)` to register connection handlers
  - Update the destructured return from setupSocketIO to include terminalNs
  - Export terminalNs for potential use by other modules

  **node-ssh shell API reference:**
  ```typescript
  const ssh = await getSSHConnection(host);
  const shell = await ssh.requestShell({ term: 'xterm-256color' });
  // shell is a Duplex stream (ClientChannel)
  shell.on('data', (data: Buffer) => { socket.emit('data', data.toString('utf-8')); });
  shell.on('close', () => { socket.emit('exit', { code: 0 }); cleanupSession(socket.id); });
  socket.on('data', (data: string) => { shell.write(data); });
  socket.on('resize', ({ cols, rows }: { cols: number; rows: number }) => {
    shell.setWindow(rows, cols, 0, 0);
  });
  ```

  Note: `ssh.requestShell()` returns a ClientChannel (Duplex stream). The `setWindow` method on ClientChannel handles PTY resize.
  </action>
  <verify>
  Run `cd /root/jarvis-backend && npx tsc --noEmit` -- must compile with zero errors.
  Verify `src/realtime/terminal.ts` exists and is >= 80 lines.
  Verify `src/realtime/terminal.ts` imports `getSSHConnection` from the SSH client (not creating raw connections).
  Verify `src/realtime/terminal.ts` does NOT call `ssh.dispose()` on pooled connections.
  Verify `src/realtime/socket.ts` creates /terminal namespace.
  Verify `src/index.ts` imports and calls setupTerminalHandlers.
  </verify>
  <done>
  /terminal Socket.IO namespace creates SSH PTY sessions using the shared SSH connection pool. Client can connect, select a node, and get a working bidirectional shell. Shell input/output streams through Socket.IO data events. PTY resize is supported. Sessions are tracked and cleaned up on disconnect without orphaning connections -- shell channels are closed but pooled SSH connections are preserved for reuse.
  </done>
</task>

</tasks>

<verification>
1. `cd /root/jarvis-backend && npx tsc --noEmit` passes with zero errors
2. `src/realtime/emitter.ts` exists with polling logic for nodes (10s), VMs (15s), storage (30s), temperature (30s)
3. `src/realtime/emitter.ts` exports `emitNodesNow` and `emitStorageNow` for on-demand data refresh
4. `src/realtime/terminal.ts` exists with SSH PTY session management using `getSSHConnection()` from pool
5. `src/realtime/terminal.ts` properly cleans up shell channels without disposing pooled SSH connections
6. `src/api/routes.ts` has POST /api/tools/execute endpoint that calls `emitNodesNow()` after successful execution
7. `src/realtime/socket.ts` creates /cluster, /events, and /terminal namespaces
8. `src/index.ts` starts emitter on boot and calls setupTerminalHandlers
9. `src/config.ts` includes dev CORS origins
</verification>

<success_criteria>
The backend emits live cluster data to connected Socket.IO clients on timed intervals. The POST /api/tools/execute endpoint allows the dashboard to invoke MCP tools (start/stop VMs, etc.) with safety tier enforcement and immediately pushes updated data to all connected clients so changes are visible within milliseconds, not up to 15 seconds. The /terminal namespace supports SSH PTY sessions using the connection pool with proper cleanup semantics. All additions compile cleanly and integrate with the existing Phase 1 architecture.
</success_criteria>

<output>
After completion, create `.planning/phases/02-real-time-dashboard-edex-ui-visual-identity/02-02-SUMMARY.md`
</output>
