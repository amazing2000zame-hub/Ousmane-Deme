---
phase: 24-observability-context-management
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - jarvis-backend/src/realtime/timing.ts
  - jarvis-backend/src/ai/local-llm.ts
  - jarvis-backend/src/ai/context-manager.ts
  - jarvis-backend/src/config.ts
autonomous: true

must_haves:
  truths:
    - "RequestTimer can record named marks and produce a relative-time breakdown"
    - "tokenize() returns accurate token counts from the Qwen tokenizer endpoint"
    - "tokenize() falls back to character estimation when endpoint is unreachable"
    - "ContextManager tracks recent messages, summary, and preserved entities per session"
    - "Background summarization produces a narrative summary plus extracted entities"
    - "Token budget enforces total context window minus system prompt minus response reserve"
    - "Config values reflect actual server context window (8192 conservative)"
  artifacts:
    - path: "jarvis-backend/src/realtime/timing.ts"
      provides: "RequestTimer class with mark() and breakdown() methods"
      exports: ["RequestTimer", "TimingBreakdown"]
    - path: "jarvis-backend/src/ai/local-llm.ts"
      provides: "tokenize() and countMessagesTokens() utility functions"
      exports: ["tokenize", "countMessagesTokens"]
    - path: "jarvis-backend/src/ai/context-manager.ts"
      provides: "ContextManager class with session-scoped sliding window and summarization"
      exports: ["ContextManager", "SessionContext"]
    - path: "jarvis-backend/src/config.ts"
      provides: "Context management and timing configuration values"
      contains: "contextWindowTokens"
  key_links:
    - from: "jarvis-backend/src/ai/context-manager.ts"
      to: "jarvis-backend/src/ai/local-llm.ts"
      via: "imports tokenize() for accurate token counting"
      pattern: "import.*tokenize.*from.*local-llm"
    - from: "jarvis-backend/src/ai/context-manager.ts"
      to: "jarvis-backend/src/config.ts"
      via: "reads contextWindowTokens, contextSummarizeThreshold, contextRecentRatio"
      pattern: "config\\.context"
    - from: "jarvis-backend/src/ai/context-manager.ts"
      to: "http://192.168.1.50:8080/v1/chat/completions"
      via: "Qwen chat completions for background summarization"
      pattern: "fetch.*chat/completions"
---

<objective>
Build the infrastructure modules for Phase 24: a RequestTimer class for pipeline timing, a tokenize() utility for accurate Qwen token counting, a ContextManager class for sliding window conversation management with background summarization, and updated config values.

Purpose: These are the building blocks that Plan 02 will wire into the live chat pipeline. Keeping them separate ensures each module is self-contained and testable before integration.
Output: Four files -- timing.ts (new), local-llm.ts (modified with tokenize), context-manager.ts (new), config.ts (modified with context config)
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/24-observability-context-management/24-RESEARCH.md
@jarvis-backend/src/config.ts
@jarvis-backend/src/ai/local-llm.ts
@jarvis-backend/src/realtime/chat.ts
@jarvis-backend/src/ai/memory-context.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: RequestTimer class and tokenize utility</name>
  <files>jarvis-backend/src/realtime/timing.ts, jarvis-backend/src/ai/local-llm.ts</files>
  <action>
**1. Create `jarvis-backend/src/realtime/timing.ts`:**

Create a `RequestTimer` class that records `performance.now()` marks at named pipeline stages and produces a timing breakdown.

```typescript
export interface TimingBreakdown {
  t0_received: number;       // 0 (base)
  t1_routed: number;         // ms after t0: routing decision complete
  t2_llm_start: number;      // ms after t0: LLM request dispatched
  t3_first_token: number;    // ms after t0: first token received from LLM
  t4_llm_done: number;       // ms after t0: LLM stream complete
  t5_tts_queued?: number;    // ms after t0: first sentence queued for TTS (voice only)
  t6_tts_first?: number;     // ms after t0: first audio chunk ready (voice only)
  t7_audio_delivered?: number; // ms after t0: first audio emitted to client (voice only)
  total_ms: number;           // t0 to final mark
}
```

- Constructor calls `this.mark('t0_received')` automatically
- `mark(name: string): void` records `performance.now()` for the given name
- `breakdown(): TimingBreakdown` returns all marks as milliseconds relative to t0
  - Missing optional marks (t5-t7) should be `undefined`, not 0
  - `total_ms` = latest mark minus t0
- `toLog(): string` returns a single-line human-readable summary like:
  `"[Timing] route=12ms llm_start=15ms first_token=890ms llm_done=2340ms tts_queued=920ms total=2450ms"`
  - Only include marks that have been recorded
  - Round to integers for readability

**2. Add `tokenize()` and `countMessagesTokens()` to `jarvis-backend/src/ai/local-llm.ts`:**

Add two exported functions at the bottom of the file (after `runLocalChat`). Do NOT modify the existing `runLocalChat` function.

```typescript
/**
 * Count tokens using the Qwen tokenizer endpoint.
 * Falls back to character-based estimation if endpoint is unreachable.
 */
export async function tokenize(text: string): Promise<number> {
  try {
    const res = await fetch(`${config.localLlmEndpoint}/tokenize`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ content: text }),
      signal: AbortSignal.timeout(2000),
    });
    if (!res.ok) return Math.ceil(text.length / 4);
    const data = await res.json() as { tokens: number[] };
    return data.tokens.length;
  } catch {
    return Math.ceil(text.length / 4);
  }
}

/**
 * Count tokens for an array of chat messages.
 * Includes ~4 token overhead per message for chat template framing.
 */
export async function countMessagesTokens(
  messages: Array<{ role: string; content: string }>,
): Promise<number> {
  const counts = await Promise.all(messages.map(m => tokenize(m.content)));
  const contentTotal = counts.reduce((a, b) => a + b, 0);
  return contentTotal + messages.length * 4; // chat template overhead
}
```

The `config` import already exists in local-llm.ts. No new imports needed.
  </action>
  <verify>
Run `cd /root/jarvis-backend && npx tsc --noEmit` to verify TypeScript compiles without errors. Verify the new files exist: `ls src/realtime/timing.ts src/ai/local-llm.ts`.
  </verify>
  <done>
timing.ts exports RequestTimer and TimingBreakdown. local-llm.ts exports tokenize() and countMessagesTokens() alongside existing runLocalChat(). TypeScript compiles cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: ContextManager class and config updates</name>
  <files>jarvis-backend/src/ai/context-manager.ts, jarvis-backend/src/config.ts</files>
  <action>
**1. Update `jarvis-backend/src/config.ts`:**

Add the following config values AFTER the existing `ttsModel` line (in the TTS section), grouped under a new comment block:

```typescript
  // Phase 24: Context management
  contextWindowTokens: parseInt(process.env.CONTEXT_WINDOW_TOKENS || '8192', 10),
  contextResponseReserve: parseInt(process.env.CONTEXT_RESPONSE_RESERVE || '1024', 10),
  contextSummarizeThreshold: parseInt(process.env.CONTEXT_SUMMARIZE_THRESHOLD || '25', 10),
  contextRecentRatio: parseFloat(process.env.CONTEXT_RECENT_RATIO || '0.7'),
  contextMaxSummaryTokens: parseInt(process.env.CONTEXT_MAX_SUMMARY_TOKENS || '500', 10),
```

Explanation of each:
- `contextWindowTokens: 8192` -- conservative effective context window (from /props n_ctx, not the -c 16384 flag; research confirms 8192 per slot)
- `contextResponseReserve: 1024` -- tokens reserved for LLM response generation
- `contextSummarizeThreshold: 25` -- trigger summarization when message count exceeds this
- `contextRecentRatio: 0.7` -- 70% of available token budget goes to recent messages, 30% to summary
- `contextMaxSummaryTokens: 500` -- hard cap on summary size to prevent summary bloat

Also update `qwenContextWindow` default from `'4096'` to `'8192'`:
```typescript
  qwenContextWindow: parseInt(process.env.QWEN_CONTEXT_WINDOW || '8192', 10),
```

**2. Create `jarvis-backend/src/ai/context-manager.ts`:**

This is the core module. It manages per-session conversation context with sliding window and background summarization.

**Exports:**
- `ContextManager` class
- `SessionContext` interface

**SessionContext interface:**
```typescript
export interface SessionContext {
  recentMessages: Array<{ role: string; content: string }>;
  summary: string | null;
  entities: Map<string, string>; // key -> description (e.g., "vm_103" -> "management VM on pve node")
  tokenCount: number;
  summarizing: boolean;
  totalMessageCount: number; // total messages seen in this session (not just recent)
}
```

**ContextManager class:**

```typescript
export class ContextManager {
  private sessions = new Map<string, SessionContext>();
```

Methods:

a) `getOrCreateSession(sessionId: string): SessionContext`
   - Returns existing session or creates a new one with empty recentMessages, null summary, empty entities, tokenCount 0, summarizing false, totalMessageCount 0.

b) `addMessage(sessionId: string, role: string, content: string): void`
   - Gets or creates session, pushes message to recentMessages, increments totalMessageCount.
   - Does NOT trigger summarization here (that happens in `shouldSummarize` + `summarize`).

c) `shouldSummarize(sessionId: string): boolean`
   - Returns true if: session exists AND totalMessageCount > config.contextSummarizeThreshold AND NOT currently summarizing.
   - Returns false otherwise.

d) `async buildContextMessages(sessionId: string, systemPromptTokens: number, memoryContextTokens: number): Promise<Array<{ role: string; content: string }>>`
   - Core method. Computes available token budget:
     ```
     availableTokens = config.contextWindowTokens - systemPromptTokens - memoryContextTokens - config.contextResponseReserve
     summaryBudget = Math.floor(availableTokens * (1 - config.contextRecentRatio))  // 30%
     recentBudget = availableTokens - summaryBudget  // 70%
     ```
   - Builds messages array:
     1. If `session.summary` exists, prepend a system message: `<conversation_summary>\n${session.summary}\n</conversation_summary>`
       - Truncate summary to `summaryBudget` tokens (use `tokenize()` to count; if over budget, truncate by character ratio)
     2. If `session.entities.size > 0`, prepend a system message: `<preserved_context>\n${entityBlock}\n</preserved_context>`
       - Entity block format: one line per entity `- key: description`
     3. Add recent messages from the END of `session.recentMessages`, fitting as many as possible within `recentBudget`
       - Work backwards from most recent: count tokens per message, stop when budget exceeded
       - Always include at least the latest message (even if it exceeds budget -- user must see their own message echoed)
   - Returns the assembled messages array.

e) `async summarize(sessionId: string): Promise<void>`
   - Called externally when `shouldSummarize()` returns true. Sets `session.summarizing = true`.
   - Selects messages to summarize: `session.recentMessages.slice(0, -config.qwenHistoryLimit)` (keep last N as recent, summarize the rest)
   - Builds a summarization prompt (see below) and calls Qwen via fetch to `${config.localLlmEndpoint}/v1/chat/completions` (non-streaming, temperature 0.3, max_tokens 512).
   - Parses the response to extract:
     - Narrative summary (text before `---ENTITIES---` marker)
     - Entities (lines after `---ENTITIES---` marker, format `key: description`)
   - Updates session:
     - If previous summary exists, prepend it to the messages being summarized (so prior summary context feeds into new summary)
     - `session.summary = narrativeSummary`
     - Merge new entities into `session.entities` (new values overwrite old for same key)
     - Remove summarized messages from `session.recentMessages` (keep only the last N that were NOT summarized)
     - Update `session.tokenCount` via `countMessagesTokens(session.recentMessages)`
   - On error: log warning, set `session.summarizing = false`, do NOT modify session state.
   - Finally: set `session.summarizing = false`.

f) `clearSession(sessionId: string): void`
   - Deletes session from the map.

**Summarization prompt (constant in the module):**

```typescript
const SUMMARIZE_SYSTEM = `You are a conversation summarizer for JARVIS, a Proxmox homelab AI assistant.`;

const SUMMARIZE_PROMPT = `Summarize the following conversation concisely.

RULES:
1. Write a narrative summary under 150 words focusing on: decisions made, problems discussed, actions taken, current discussion state
2. Preserve ALL specific identifiers verbatim: VMIDs (like VM 103), IP addresses (like 192.168.1.50), node names (Home, pve, agent1, agent), file paths, container names, error messages
3. After the summary, output preserved entities on separate lines after a ---ENTITIES--- marker
4. Entity format: key: description (one per line)

FORMAT EXAMPLE:
User discussed VM 103 on pve node having high CPU. JARVIS ran diagnostics and found a runaway process. User asked to restart the VM.

---ENTITIES---
vm_103: management VM on pve node (192.168.1.65)
node_pve: 192.168.1.74, compute + NAS node
error_discussed: high CPU usage from runaway process on VM 103

CONVERSATION TO SUMMARIZE:
`;
```

**Important implementation notes:**
- The summarization fetch uses `stream: false` (NOT streaming) for simplicity -- we need the full response to parse entities.
- Use `AbortSignal.timeout(15000)` for the summarization call (15 second timeout -- it's background work).
- The summarization call does NOT use the agentic loop or providers -- it directly calls the Qwen endpoint. This avoids consuming the provider abstraction layer.
- Import `tokenize` and `countMessagesTokens` from `'./local-llm.js'`.
- Import `config` from `'../config.js'`.
  </action>
  <verify>
Run `cd /root/jarvis-backend && npx tsc --noEmit` to verify TypeScript compiles. Verify the new file exists: `ls src/ai/context-manager.ts`. Verify config has new values: `grep contextWindowTokens src/config.ts`.
  </verify>
  <done>
context-manager.ts exports ContextManager class and SessionContext interface. Config has 5 new context management values and updated qwenContextWindow default to 8192. TypeScript compiles cleanly. No runtime behavior changes yet (integration is Plan 02).
  </done>
</task>

</tasks>

<verification>
1. `cd /root/jarvis-backend && npx tsc --noEmit` -- TypeScript compilation passes
2. All 4 files exist: `timing.ts` (new), `local-llm.ts` (modified), `context-manager.ts` (new), `config.ts` (modified)
3. No runtime changes -- existing chat behavior unaffected (integration deferred to Plan 02)
4. `tokenize()` handles both success and failure gracefully (fallback to character estimation)
5. `RequestTimer` produces correct relative timestamps
</verification>

<success_criteria>
- RequestTimer class exists with mark(), breakdown(), and toLog() methods
- tokenize() calls /tokenize endpoint with 2s timeout and character fallback
- countMessagesTokens() aggregates token counts with template overhead
- ContextManager manages per-session state with addMessage, buildContextMessages, shouldSummarize, summarize
- Config has contextWindowTokens=8192, contextResponseReserve=1024, contextSummarizeThreshold=25, contextRecentRatio=0.7, contextMaxSummaryTokens=500
- qwenContextWindow updated from 4096 to 8192
- TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/24-observability-context-management/24-01-SUMMARY.md`
</output>
