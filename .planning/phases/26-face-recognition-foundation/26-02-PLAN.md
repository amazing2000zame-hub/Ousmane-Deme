---
phase: 26-face-recognition-foundation
plan: 02
type: execute
wave: 2
depends_on: ["26-01"]
files_modified:
  - jarvis-backend/src/mcp/tools/smarthome.ts
  - jarvis-backend/src/safety/tiers.ts
autonomous: true

must_haves:
  truths:
    - "User can ask 'Who's at the door?' and JARVIS returns recognized face name"
    - "User can query all recognized face events"
    - "User can query unknown visitors (person without face recognition)"
  artifacts:
    - path: "jarvis-backend/src/mcp/tools/smarthome.ts"
      provides: "3 new MCP tools for face recognition queries"
      exports: ["whos_at_door", "get_recognized_faces", "get_unknown_visitors"]
    - path: "jarvis-backend/src/safety/tiers.ts"
      provides: "Safety tier mappings for new tools"
      contains: "whos_at_door: ActionTier.GREEN"
  key_links:
    - from: "smarthome.ts whos_at_door"
      to: "frigate.ts getRecentFaceEvents"
      via: "function call"
      pattern: "getRecentFaceEvents"
    - from: "smarthome.ts get_unknown_visitors"
      to: "frigate.ts getEvents"
      via: "filter sub_label null"
      pattern: "sub_label.*null"
---

<objective>
Add 3 new MCP tools for face recognition queries: whos_at_door, get_recognized_faces, and get_unknown_visitors.

Purpose: Enable JARVIS to answer face-related questions like "Who's at the door?", "Who visited today?", and "Were there any unknown visitors?". All tools are GREEN tier (read-only, no side effects).

Output: 3 new MCP tools registered in smarthome.ts with GREEN tier safety classification.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/milestones/v1.6-ROADMAP.md
@.planning/phases/26-face-recognition-foundation/26-01-SUMMARY.md
@jarvis-backend/src/clients/frigate.ts
@jarvis-backend/src/mcp/tools/smarthome.ts
@jarvis-backend/src/safety/tiers.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add whos_at_door MCP tool</name>
  <files>jarvis-backend/src/mcp/tools/smarthome.ts</files>
  <action>
    Add new MCP tool `whos_at_door` after the existing `query_nvr_detections` tool:

    ```typescript
    // 10. whos_at_door -- check for recent person at entry cameras with face recognition
    server.tool(
      'whos_at_door',
      'Check who is at the door by querying recent person detections with face recognition at entry cameras (front_door)',
      {
        withinMinutes: z.number().min(1).max(60).optional()
          .describe('Look back N minutes (default: 5)'),
      },
      async ({ withinMinutes }) => {
        try {
          const lookbackMinutes = withinMinutes ?? 5;
          const after = Math.floor(Date.now() / 1000) - lookbackMinutes * 60;

          // Query person events at entry cameras (front_door)
          const events = await frigate.getEvents({
            camera: 'front_door',
            label: 'person',
            after,
            limit: 10,
            has_snapshot: true,
          });

          if (events.length === 0) {
            return {
              content: [{
                type: 'text' as const,
                text: JSON.stringify({
                  status: 'no_one',
                  message: `No one detected at the door in the last ${lookbackMinutes} minutes`,
                  events: [],
                }, null, 2),
              }],
            };
          }

          // Parse face recognition data
          const visitors = events.map((e) => {
            const face = frigate.parseFaceSubLabel(e.sub_label);
            return {
              time: new Date(e.start_time * 1000).toLocaleString(),
              recognized: face.name !== null,
              name: face.name ?? 'Unknown person',
              confidence: face.confidence ? `${(face.confidence * 100).toFixed(0)}%` : null,
              eventId: e.id,
              hasSnapshot: e.has_snapshot,
            };
          });

          // Build summary
          const recognized = visitors.filter(v => v.recognized);
          const unknown = visitors.filter(v => !v.recognized);

          let summary: string;
          if (recognized.length > 0) {
            const names = [...new Set(recognized.map(v => v.name))];
            summary = `${names.join(', ')} at the door`;
            if (unknown.length > 0) {
              summary += ` (plus ${unknown.length} unknown person${unknown.length > 1 ? 's' : ''})`;
            }
          } else {
            summary = `${unknown.length} unknown person${unknown.length > 1 ? 's' : ''} at the door`;
          }

          return {
            content: [{
              type: 'text' as const,
              text: JSON.stringify({
                status: 'detected',
                summary,
                visitors,
                lookbackMinutes,
              }, null, 2),
            }],
          };
        } catch (err) {
          return {
            content: [{ type: 'text' as const, text: `Error: ${err instanceof Error ? err.message : String(err)}` }],
            isError: true,
          };
        }
      },
    );
    ```

    This tool specifically queries front_door camera for person events and parses face recognition data.
    Returns structured response with recognized names and unknown visitors.
  </action>
  <verify>npm run build passes in jarvis-backend</verify>
  <done>whos_at_door tool registered, queries front_door camera for person events with face data</done>
</task>

<task type="auto">
  <name>Task 2: Add get_recognized_faces and get_unknown_visitors tools</name>
  <files>jarvis-backend/src/mcp/tools/smarthome.ts</files>
  <action>
    Add two more MCP tools after whos_at_door:

    ```typescript
    // 11. get_recognized_faces -- list all face events with recognized names
    server.tool(
      'get_recognized_faces',
      'Get recent events where a face was recognized (identified by name)',
      {
        camera: z.string().optional().describe('Filter by camera name'),
        name: z.string().optional().describe('Filter by person name'),
        limit: z.number().min(1).max(50).optional().describe('Max results (default: 20)'),
        withinMinutes: z.number().min(1).max(1440).optional().describe('Look back N minutes (default: 60)'),
      },
      async ({ camera, name, limit, withinMinutes }) => {
        try {
          const lookbackMinutes = withinMinutes ?? 60;
          const after = Math.floor(Date.now() / 1000) - lookbackMinutes * 60;

          const events = await frigate.getEvents({
            camera,
            label: 'person',
            after,
            limit: limit ?? 20,
            has_snapshot: true,
          });

          // Filter to only events with face recognition
          const recognizedEvents = events
            .map((e) => {
              const face = frigate.parseFaceSubLabel(e.sub_label);
              return { event: e, face };
            })
            .filter((r) => r.face.name !== null)
            .filter((r) => !name || r.face.name?.toLowerCase() === name.toLowerCase());

          const formatted = recognizedEvents.map((r) => ({
            name: r.face.name,
            confidence: r.face.confidence ? `${(r.face.confidence * 100).toFixed(0)}%` : 'N/A',
            camera: r.event.camera,
            time: new Date(r.event.start_time * 1000).toLocaleString(),
            eventId: r.event.id,
            hasSnapshot: r.event.has_snapshot,
            hasClip: r.event.has_clip,
          }));

          // Group by name for summary
          const byName: Record<string, number> = {};
          for (const e of formatted) {
            byName[e.name!] = (byName[e.name!] || 0) + 1;
          }

          return {
            content: [{
              type: 'text' as const,
              text: JSON.stringify({
                totalRecognized: formatted.length,
                summary: byName,
                events: formatted,
                lookbackMinutes,
              }, null, 2),
            }],
          };
        } catch (err) {
          return {
            content: [{ type: 'text' as const, text: `Error: ${err instanceof Error ? err.message : String(err)}` }],
            isError: true,
          };
        }
      },
    );

    // 12. get_unknown_visitors -- query person events without face recognition
    server.tool(
      'get_unknown_visitors',
      'Get recent person detections where no face was recognized (unknown visitors)',
      {
        camera: z.string().optional().describe('Filter by camera name'),
        limit: z.number().min(1).max(50).optional().describe('Max results (default: 20)'),
        withinMinutes: z.number().min(1).max(1440).optional().describe('Look back N minutes (default: 60)'),
      },
      async ({ camera, limit, withinMinutes }) => {
        try {
          const lookbackMinutes = withinMinutes ?? 60;
          const after = Math.floor(Date.now() / 1000) - lookbackMinutes * 60;

          const events = await frigate.getEvents({
            camera,
            label: 'person',
            after,
            limit: limit ?? 20,
            has_snapshot: true,
          });

          // Filter to only events WITHOUT face recognition (sub_label is null)
          const unknownEvents = events.filter((e) => {
            const face = frigate.parseFaceSubLabel(e.sub_label);
            return face.name === null;
          });

          const formatted = unknownEvents.map((e) => ({
            camera: e.camera,
            time: new Date(e.start_time * 1000).toLocaleString(),
            duration: e.end_time ? `${Math.round(e.end_time - e.start_time)}s` : 'ongoing',
            confidence: `${(e.score * 100).toFixed(1)}%`,
            eventId: e.id,
            hasSnapshot: e.has_snapshot,
            hasClip: e.has_clip,
          }));

          // Group by camera for summary
          const byCamera: Record<string, number> = {};
          for (const e of formatted) {
            byCamera[e.camera] = (byCamera[e.camera] || 0) + 1;
          }

          return {
            content: [{
              type: 'text' as const,
              text: JSON.stringify({
                totalUnknown: formatted.length,
                summary: byCamera,
                events: formatted,
                lookbackMinutes,
                note: unknownEvents.length === 0
                  ? 'No unknown visitors detected'
                  : `${unknownEvents.length} person detection(s) without face recognition`,
              }, null, 2),
            }],
          };
        } catch (err) {
          return {
            content: [{ type: 'text' as const, text: `Error: ${err instanceof Error ? err.message : String(err)}` }],
            isError: true,
          };
        }
      },
    );
    ```

    Also update the module docstring to reflect 12 tools:
    ```typescript
    /**
     * 12 smart home control tools for presence detection, thermostat, locks, cameras, and face recognition.
     * ...
     */
    ```
  </action>
  <verify>npm run build passes in jarvis-backend</verify>
  <done>get_recognized_faces and get_unknown_visitors tools registered</done>
</task>

<task type="auto">
  <name>Task 3: Register tools in safety tiers</name>
  <files>jarvis-backend/src/safety/tiers.ts</files>
  <action>
    Add the 3 new tools to TOOL_TIERS with GREEN tier (read-only):

    In the "GREEN -- read-only smart home" section, add:
    ```typescript
    // GREEN -- read-only smart home
    get_who_is_home: ActionTier.GREEN,
    scan_network_devices: ActionTier.GREEN,
    get_thermostat_status: ActionTier.GREEN,
    get_lock_status: ActionTier.GREEN,
    get_camera_snapshot: ActionTier.GREEN,
    query_nvr_detections: ActionTier.GREEN,

    // GREEN -- face recognition queries (Phase 26)
    whos_at_door: ActionTier.GREEN,
    get_recognized_faces: ActionTier.GREEN,
    get_unknown_visitors: ActionTier.GREEN,
    ```

    Also update the header comment in smarthome.ts to include the new tools:
    ```typescript
    /**
     * Safety tiers:
     *   GREEN: get_who_is_home, get_thermostat_status, get_lock_status,
     *          get_camera_snapshot, query_nvr_detections, scan_network_devices,
     *          whos_at_door, get_recognized_faces, get_unknown_visitors
     *   YELLOW: set_thermostat
     *   RED: lock_door, unlock_door
     */
    ```
  </action>
  <verify>npm run build passes, all 3 new tools mapped to GREEN tier</verify>
  <done>whos_at_door, get_recognized_faces, get_unknown_visitors all classified as GREEN tier</done>
</task>

</tasks>

<verification>
1. `npm run build` passes in jarvis-backend
2. All 3 new tools are exported and callable
3. Safety tiers correctly map all 3 tools to GREEN
4. Tool descriptions are clear for Claude to understand when to use each
</verification>

<success_criteria>
- whos_at_door tool queries front_door camera and returns face-annotated results
- get_recognized_faces tool returns all events with face recognition data
- get_unknown_visitors tool returns person events without face recognition
- All 3 tools are GREEN tier (read-only, auto-execute)
- Backend builds and runs without errors
</success_criteria>

<output>
After completion, create `.planning/phases/26-face-recognition-foundation/26-02-SUMMARY.md`
</output>
